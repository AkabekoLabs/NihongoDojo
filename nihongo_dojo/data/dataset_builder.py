"""
Dataset builder and management interface for Nihongo DoJo
Provides easy-to-use APIs for loading pre-generated datasets
"""

import os
import json
import requests
from typing import Dict, List, Optional, Union, Tuple
from pathlib import Path
from tqdm import tqdm
import hashlib
from urllib.parse import urljoin

from .large_scale_datasets import (
    DatasetLoader, 
    DatasetSerializer,
    DatasetMetadata,
    generate_preset_dataset,
    PRESET_DATASETS
)


class NihongoDojoDatasetHub:
    """
    Nihongo DoJoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ–
    äº‹å‰ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®ç°¡å˜ãªã‚¢ã‚¯ã‚»ã‚¹ã‚’æä¾›
    """
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒªãƒã‚¸ãƒˆãƒªURLï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã«æ›´æ–°ï¼‰
    DEFAULT_REPO_URL = "https://huggingface.co/datasets/nihongo-dojo/grpo-datasets/resolve/main/"
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    AVAILABLE_DATASETS = {
        "nihongo-dojo-10k": {
            "size": 10000,
            "description": "åŸºæœ¬çš„ãª10,000ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ†ã‚¹ãƒˆãƒ»é–‹ç™ºç”¨ï¼‰",
            "url": "nihongo-dojo-10k.tar.gz",
            "checksum": "placeholder_checksum_10k",
            "compressed_size": "~50MB"
        },
        "nihongo-dojo-50k": {
            "size": 50000,
            "description": "ä¸­è¦æ¨¡50,000ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆå°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ï¼‰",
            "url": "nihongo-dojo-50k.tar.gz",
            "checksum": "placeholder_checksum_50k",
            "compressed_size": "~250MB"
        },
        "nihongo-dojo-100k": {
            "size": 100000,
            "description": "æ¨™æº–100,000ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ¨å¥¨ï¼‰",
            "url": "nihongo-dojo-100k.tar.gz",
            "checksum": "placeholder_checksum_100k",
            "compressed_size": "~500MB"
        },
        "nihongo-dojo-500k": {
            "size": 500000,
            "description": "å¤§è¦æ¨¡500,000ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰",
            "url": "nihongo-dojo-500k.tar.gz",
            "checksum": "placeholder_checksum_500k",
            "compressed_size": "~2.5GB"
        },
        "nihongo-dojo-beginner": {
            "size": 100000,
            "description": "åˆç´šãƒ¬ãƒ™ãƒ«ç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "url": "nihongo-dojo-beginner-100k.tar.gz",
            "checksum": "placeholder_checksum_beginner",
            "compressed_size": "~400MB"
        },
        "nihongo-dojo-business": {
            "size": 50000,
            "description": "ãƒ“ã‚¸ãƒã‚¹æ—¥æœ¬èªç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "url": "nihongo-dojo-business-50k.tar.gz",
            "checksum": "placeholder_checksum_business",
            "compressed_size": "~300MB"
        }
    }
    
    def __init__(self, cache_dir: Optional[str] = None, repo_url: Optional[str] = None):
        """
        Initialize dataset hub
        
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ~/.cache/nihongo_dojoï¼‰
            repo_url: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒªãƒã‚¸ãƒˆãƒªURL
        """
        self.cache_dir = Path(cache_dir or os.path.expanduser("~/.cache/nihongo_dojo"))
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.repo_url = repo_url or self.DEFAULT_REPO_URL
    
    def list_datasets(self) -> Dict[str, Dict]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã‚’å–å¾—"""
        return self.AVAILABLE_DATASETS.copy()
    
    def load_dataset(
        self,
        name: str,
        force_download: bool = False,
        verify_checksum: bool = True
    ) -> DatasetLoader:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            force_download: å¼·åˆ¶çš„ã«å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹
            verify_checksum: ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’æ¤œè¨¼ã™ã‚‹ã‹
            
        Returns:
            DatasetLoader ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        if name not in self.AVAILABLE_DATASETS:
            raise ValueError(f"Unknown dataset: {name}. Available: {list(self.AVAILABLE_DATASETS.keys())}")
        
        dataset_info = self.AVAILABLE_DATASETS[name]
        dataset_path = self.cache_dir / name
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ã‹ç¢ºèª
        if force_download or not dataset_path.exists():
            self._download_dataset(name, dataset_info, verify_checksum)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¿”ã™
        return DatasetLoader(str(dataset_path))
    
    def _download_dataset(
        self,
        name: str,
        dataset_info: Dict,
        verify_checksum: bool = True
    ):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        import tarfile
        
        dataset_url = urljoin(self.repo_url, dataset_info["url"])
        dataset_path = self.cache_dir / name
        archive_path = self.cache_dir / f"{name}.tar.gz"
        
        print(f"ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {name}")
        print(f"   URL: {dataset_url}")
        print(f"   ã‚µã‚¤ã‚º: {dataset_info['compressed_size']}")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = requests.get(dataset_url, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        
        with open(archive_path, 'wb') as f:
            with tqdm(total=total_size, unit='B', unit_scale=True, desc="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰") as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    pbar.update(len(chunk))
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼
        if verify_checksum:
            print("ğŸ” ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼ä¸­...")
            calculated_checksum = self._calculate_checksum(archive_path)
            if calculated_checksum != dataset_info["checksum"]:
                os.remove(archive_path)
                raise ValueError(f"Checksum mismatch for {name}")
        
        # è§£å‡
        print("ğŸ“¦ è§£å‡ä¸­...")
        with tarfile.open(archive_path, 'r:gz') as tar:
            tar.extractall(self.cache_dir)
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‰Šé™¤
        os.remove(archive_path)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™å®Œäº†: {dataset_path}")
    
    def _calculate_checksum(self, filepath: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’è¨ˆç®—"""
        sha256_hash = hashlib.sha256()
        with open(filepath, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def get_dataset_info(self, name: str) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        if name not in self.AVAILABLE_DATASETS:
            raise ValueError(f"Unknown dataset: {name}")
        
        info = self.AVAILABLE_DATASETS[name].copy()
        dataset_path = self.cache_dir / name
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if dataset_path.exists():
            info["local_path"] = str(dataset_path)
            info["is_downloaded"] = True
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            try:
                metadata_path = dataset_path / "metadata.json"
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                    info["metadata"] = metadata
            except:
                pass
        else:
            info["is_downloaded"] = False
        
        return info


def load_dataset(
    name: str = "nihongo-dojo-100k",
    cache_dir: Optional[str] = None,
    streaming: bool = False,
    batch_size: int = 32
) -> Union[DatasetLoader, 'StreamingDataset']:
    """
    Nihongo DoJoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ä¾¿åˆ©é–¢æ•°
    
    Args:
        name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        streaming: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã‚€ã‹
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚ï¼‰
        
    Returns:
        DatasetLoaderã¾ãŸã¯StreamingDatasetã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        
    Example:
        >>> # æ¨™æº–çš„ãªä½¿ã„æ–¹
        >>> dataset = load_dataset("nihongo-dojo-100k")
        >>> for batch in dataset.iter_batches(batch_size=32):
        ...     # ãƒãƒƒãƒå‡¦ç†
        ...     pass
        
        >>> # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰
        >>> dataset = load_dataset("nihongo-dojo-100k", streaming=True)
        >>> for batch in dataset:
        ...     # ãƒãƒƒãƒå‡¦ç†
        ...     pass
    """
    hub = NihongoDojoDatasetHub(cache_dir=cache_dir)
    
    if streaming:
        return StreamingDataset(hub, name, batch_size)
    else:
        return hub.load_dataset(name)


class StreamingDataset:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ãƒƒãƒ‘ãƒ¼"""
    
    def __init__(self, hub: NihongoDojoDatasetHub, name: str, batch_size: int = 32):
        self.hub = hub
        self.name = name
        self.batch_size = batch_size
        self._loader = None
    
    def __iter__(self):
        """ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’è¿”ã™"""
        if self._loader is None:
            self._loader = self.hub.load_dataset(self.name)
        return self._loader.iter_batches(batch_size=self.batch_size, shuffle=True)
    
    def __len__(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’è¿”ã™"""
        if self._loader is None:
            self._loader = self.hub.load_dataset(self.name)
        return len(self._loader)


# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
def create_all_preset_datasets(output_base_dir: str = "./datasets"):
    """
    å…¨ã¦ã®ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
    
    Args:
        output_base_dir: å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    from .large_scale_datasets import LargeScaleDatasetGenerator, DatasetSerializer
    
    # ç”Ÿæˆè¨­å®š
    configs = {
        "nihongo-dojo-10k": {
            "total_tasks": 10000,
            "task_type_distribution": {'basic': 0.7, 'advanced': 0.2, 'cultural': 0.1}
        },
        "nihongo-dojo-50k": {
            "total_tasks": 50000,
            "task_type_distribution": {'basic': 0.6, 'advanced': 0.3, 'cultural': 0.1}
        },
        "nihongo-dojo-100k": {
            "total_tasks": 100000,
            "task_type_distribution": {'basic': 0.6, 'advanced': 0.3, 'cultural': 0.1}
        },
        "nihongo-dojo-500k": {
            "total_tasks": 500000,
            "task_type_distribution": {'basic': 0.5, 'advanced': 0.35, 'cultural': 0.15}
        },
        "nihongo-dojo-beginner": {
            "total_tasks": 100000,
            "task_type_distribution": {'basic': 0.9, 'advanced': 0.1, 'cultural': 0.0},
            "difficulty_distribution": {
                "beginner": 0.7,
                "intermediate": 0.3,
                "advanced": 0.0,
                "native": 0.0
            }
        },
        "nihongo-dojo-business": {
            "total_tasks": 50000,
            "task_type_distribution": {'basic': 0.2, 'advanced': 0.7, 'cultural': 0.1},
            "focus_tasks": ["business_japanese", "keigo_conversion", "formal_writing"]
        }
    }
    
    generator = LargeScaleDatasetGenerator()
    
    for name, config in configs.items():
        print(f"\n{'='*70}")
        print(f"ğŸ¯ ç”Ÿæˆé–‹å§‹: {name}")
        print(f"{'='*70}")
        
        output_dir = os.path.join(output_base_dir, name)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
        data, metadata = generator.generate_large_dataset(**config)
        
        # ä¿å­˜
        DatasetSerializer.save_dataset(
            data=data,
            metadata=metadata,
            output_dir=output_dir,
            format="jsonl",
            compress=True
        )
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ
        import tarfile
        archive_path = f"{output_dir}.tar.gz"
        with tarfile.open(archive_path, 'w:gz') as tar:
            tar.add(output_dir, arcname=name)
        
        print(f"âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆå®Œäº†: {archive_path}")
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
        checksum = hashlib.sha256()
        with open(archive_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                checksum.update(chunk)
        
        print(f"ğŸ“ ãƒã‚§ãƒƒã‚¯ã‚µãƒ : {checksum.hexdigest()}")


# ç°¡æ˜“ã‚¢ã‚¯ã‚»ã‚¹ç”¨ã®ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ
def load_small_dataset(**kwargs):
    """å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ10kï¼‰ã‚’èª­ã¿è¾¼ã‚€"""
    return load_dataset("nihongo-dojo-10k", **kwargs)


def load_medium_dataset(**kwargs):
    """ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ50kï¼‰ã‚’èª­ã¿è¾¼ã‚€"""
    return load_dataset("nihongo-dojo-50k", **kwargs)


def load_large_dataset(**kwargs):
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ100kï¼‰ã‚’èª­ã¿è¾¼ã‚€"""
    return load_dataset("nihongo-dojo-100k", **kwargs)


def load_extra_large_dataset(**kwargs):
    """è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ500kï¼‰ã‚’èª­ã¿è¾¼ã‚€"""
    return load_dataset("nihongo-dojo-500k", **kwargs)