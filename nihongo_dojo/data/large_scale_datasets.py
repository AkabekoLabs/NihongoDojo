"""
Large-scale dataset generation and management for Nihongo DoJo
Supports 100k+ dataset generation for GRPO training
"""

import json
import gzip
import pickle
import hashlib
from typing import List, Dict, Optional, Union, Tuple, Iterator
from pathlib import Path
from dataclasses import dataclass, asdict
import numpy as np
from tqdm import tqdm
import multiprocessing as mp
from functools import partial
import random
from datetime import datetime

from ..core import TaskType, TaskDifficulty, NihongoDoJo
from .datasets import NihongoGRPODataset, create_mixed_dataset


@dataclass
class DatasetMetadata:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    name: str
    version: str
    created_at: str
    num_tasks: int
    num_groups: int
    task_types: List[str]
    difficulty_distribution: Dict[str, float]
    file_format: str
    compression: str
    checksum: str
    description: str


class LargeScaleDatasetGenerator:
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, num_workers: int = None):
        """
        Initialize large-scale dataset generator
        
        Args:
            num_workers: ä¸¦åˆ—å‡¦ç†ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆNoneã®å ´åˆã¯CPUæ•°-1ï¼‰
        """
        self.num_workers = num_workers or max(1, mp.cpu_count() - 1)
        self.dojo = NihongoDoJo()
        
    def generate_large_dataset(
        self,
        total_tasks: int = 100000,
        task_type_distribution: Optional[Dict[str, float]] = None,
        difficulty_distribution: Optional[Dict[TaskDifficulty, float]] = None,
        group_size: int = 4,
        batch_size: int = 1000,
        seed: int = 42
    ) -> Tuple[List[Dict], DatasetMetadata]:
        """
        å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
        
        Args:
            total_tasks: ç·ã‚¿ã‚¹ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100,000ï¼‰
            task_type_distribution: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®åˆ†å¸ƒ
            difficulty_distribution: é›£æ˜“åº¦ã®åˆ†å¸ƒ
            group_size: GRPOã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚º
            batch_size: ãƒãƒƒãƒå‡¦ç†ã‚µã‚¤ã‚º
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
            
        Returns:
            (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)ã®ã‚¿ãƒ—ãƒ«
        """
        random.seed(seed)
        np.random.seed(seed)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†å¸ƒ
        if task_type_distribution is None:
            task_type_distribution = {
                'basic': 0.6,      # åŸºæœ¬ã‚¿ã‚¹ã‚¯
                'advanced': 0.3,   # æ‹¡å¼µã‚¿ã‚¹ã‚¯
                'cultural': 0.1    # æ–‡åŒ–ã‚¿ã‚¹ã‚¯
            }
        
        if difficulty_distribution is None:
            difficulty_distribution = {
                TaskDifficulty.BEGINNER: 0.25,
                TaskDifficulty.INTERMEDIATE: 0.35,
                TaskDifficulty.ADVANCED: 0.25,
                TaskDifficulty.NATIVE: 0.15
            }
        
        print(f"ğŸš€ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆé–‹å§‹: {total_tasks:,}ã‚¿ã‚¹ã‚¯")
        print(f"   ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.num_workers}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        
        # ãƒãƒƒãƒåˆ†å‰²
        num_batches = (total_tasks + batch_size - 1) // batch_size
        batches = []
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_tasks)
            batch_tasks = end_idx - start_idx
            batches.append({
                'batch_id': i,
                'num_tasks': batch_tasks,
                'task_type_dist': task_type_distribution,
                'difficulty_dist': difficulty_distribution,
                'seed': seed + i
            })
        
        # ä¸¦åˆ—å‡¦ç†ã§ãƒãƒƒãƒç”Ÿæˆ
        all_tasks = []
        with mp.Pool(processes=self.num_workers) as pool:
            results = list(tqdm(
                pool.imap(self._generate_batch, batches),
                total=len(batches),
                desc="ãƒãƒƒãƒç”Ÿæˆ"
            ))
            
            for batch_tasks in results:
                all_tasks.extend(batch_tasks)
        
        # Noneå€¤ã‚’é™¤å¤–
        all_tasks = [task for task in all_tasks if task is not None]
        
        print(f"âœ… ã‚¿ã‚¹ã‚¯ç”Ÿæˆå®Œäº†: {len(all_tasks):,}ã‚¿ã‚¹ã‚¯")
        
        # GRPOã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
        print("ğŸ“¦ GRPOã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆä¸­...")
        groups = self._create_grpo_groups(all_tasks, group_size)
        
        # å­¦ç¿’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
        print("ğŸ”„ å­¦ç¿’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ä¸­...")
        training_data = self._convert_to_training_format(groups)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        metadata = self._create_metadata(
            name=f"nihongo_dojo_large_{total_tasks}",
            num_tasks=len(all_tasks),
            num_groups=len(groups),
            task_type_distribution=task_type_distribution,
            difficulty_distribution=difficulty_distribution,
            training_data=training_data
        )
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†!")
        print(f"   ã‚¿ã‚¹ã‚¯æ•°: {metadata.num_tasks:,}")
        print(f"   ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {metadata.num_groups:,}")
        
        return training_data, metadata
    
    def _generate_batch(self, batch_config: Dict) -> List[Dict]:
        """ãƒãƒƒãƒå˜ä½ã§ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
        random.seed(batch_config['seed'])
        dojo = NihongoDoJo()  # å„ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
        
        tasks = []
        task_type_dist = batch_config['task_type_dist']
        difficulty_dist = batch_config['difficulty_dist']
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘
        task_categories = {
            'basic': [
                TaskType.KANJI_READING,
                TaskType.KANJI_WRITING,
                TaskType.PARTICLE_FILL,
                TaskType.KEIGO_CONVERSION,
                TaskType.WORD_ORDER,
                TaskType.COUNTER_WORD,
            ],
            'advanced': [
                TaskType.ADVANCED_GRAMMAR,
                TaskType.ONOMATOPOEIA,
                TaskType.CONVERSATION,
                TaskType.PROVERB_IDIOM,
                TaskType.BUSINESS_JAPANESE,
                TaskType.CLASSICAL_JAPANESE,
                TaskType.SPECIALIZED_VOCABULARY
            ],
            'cultural': [
                TaskType.SEASONAL_EXPRESSION,
                TaskType.SOCIAL_CONTEXT,
                TaskType.REGIONAL_DIALECT,
                TaskType.AGE_GENDER_LANGUAGE,
                TaskType.EMOTIONAL_EXPRESSION
            ]
        }
        
        for _ in range(batch_config['num_tasks']):
            # ã‚«ãƒ†ã‚´ãƒªé¸æŠ
            category = np.random.choice(
                list(task_type_dist.keys()),
                p=list(task_type_dist.values())
            )
            
            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—é¸æŠ
            task_type = random.choice(task_categories[category])
            
            # é›£æ˜“åº¦é¸æŠ
            difficulty = np.random.choice(
                list(difficulty_dist.keys()),
                p=list(difficulty_dist.values())
            )
            
            # ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
            try:
                task = dojo.generate_task(task_type=task_type, difficulty=difficulty)
                tasks.append(task)
            except Exception as e:
                print(f"Warning: Failed to generate task {task_type}: {e}")
                continue
        
        return tasks
    
    def _create_grpo_groups(self, tasks: List[Dict], group_size: int) -> List[List[Dict]]:
        """GRPOã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ"""
        # Noneå€¤ã‚’é™¤å¤–ã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        valid_tasks = [task for task in tasks if task is not None]
        shuffled_tasks = valid_tasks.copy()
        random.shuffle(shuffled_tasks)
        
        # ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
        groups = []
        for i in range(0, len(shuffled_tasks), group_size):
            group = shuffled_tasks[i:i + group_size]
            if len(group) == group_size:  # å®Œå…¨ãªã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿
                groups.append(group)
        
        return groups
    
    def _convert_to_training_format(self, groups: List[List[Dict]]) -> List[Dict]:
        """å­¦ç¿’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›"""
        training_data = []
        
        for group_id, group in enumerate(groups):
            for task_idx, task in enumerate(group):
                # JapaneseTaskã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
                if hasattr(task, '__dataclass_fields__'):
                    # dataclassã®å ´åˆ
                    difficulty_value = task.difficulty.value if hasattr(task.difficulty, 'value') else task.difficulty
                    task_type_value = task.type.value if hasattr(task.type, 'value') else task.type
                    instruction = task.instruction
                    question = task.question
                    answer = task.answer
                    explanation = task.explanation or ''
                else:
                    # æ—¢ã«è¾æ›¸ã®å ´åˆ
                    difficulty_value = task['difficulty']
                    task_type_value = task['type']
                    instruction = task.get('instruction', 'æ¬¡ã®å•é¡Œã«ç­”ãˆã¦ãã ã•ã„ã€‚')
                    question = task.get('question', task.get('problem', ''))
                    answer = task.get('answer', task.get('solution', ''))
                    explanation = task.get('explanation', task.get('reasoning', ''))
                
                # é›£æ˜“åº¦ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã™ã¹ã¦çµ±ä¸€ï¼‰
                reasoning_start = "<think>"
                reasoning_end = "</think>"
                
                # å¿œç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                formatted_output = f"{reasoning_start}\n{explanation}\n{reasoning_end}\n<answer>{answer}</answer>"
                
                example = {
                    "instruction": instruction,
                    "input": question,
                    "output": formatted_output,
                    "group_id": group_id,
                    "task_idx": task_idx,
                    "task_type": task_type_value,
                    "difficulty": difficulty_value,
                    "metadata": {
                        "question": question,
                        "answer": answer,
                        "reasoning": explanation,
                        "created_at": datetime.now().isoformat()
                    }
                }
                
                training_data.append(example)
        
        return training_data
    
    def _create_metadata(
        self,
        name: str,
        num_tasks: int,
        num_groups: int,
        task_type_distribution: Dict,
        difficulty_distribution: Dict,
        training_data: List[Dict]
    ) -> DatasetMetadata:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
        data_str = json.dumps(training_data, sort_keys=True)
        checksum = hashlib.sha256(data_str.encode()).hexdigest()
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—çµ±è¨ˆ
        task_types = {}
        for item in training_data:
            task_type = item['task_type']
            task_types[task_type] = task_types.get(task_type, 0) + 1
        
        return DatasetMetadata(
            name=name,
            version="1.0.0",
            created_at=datetime.now().isoformat(),
            num_tasks=num_tasks,
            num_groups=num_groups,
            task_types=list(task_types.keys()),
            difficulty_distribution={k.value: v for k, v in difficulty_distribution.items()},
            file_format="json",
            compression="gzip",
            checksum=checksum,
            description=f"Nihongo DoJo large-scale dataset with {num_tasks:,} tasks for GRPO training"
        )


class DatasetSerializer:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ»ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
    
    @staticmethod
    def save_dataset(
        data: List[Dict],
        metadata: DatasetMetadata,
        output_dir: str,
        format: str = "jsonl",
        compress: bool = True,
        chunk_size: int = 10000
    ):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜
        
        Args:
            data: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            format: ä¿å­˜å½¢å¼ï¼ˆjson, jsonl, pickleï¼‰
            compress: åœ§ç¸®ã™ã‚‹ã‹
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ï¼‰
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata_path = output_path / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            # metadataãŒæ—¢ã«è¾æ›¸ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨ã€dataclassã®å ´åˆã¯asdict()ã‚’ä½¿ç”¨
            if hasattr(metadata, '__dataclass_fields__'):
                json.dump(asdict(metadata), f, ensure_ascii=False, indent=2)
            else:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ä¸­: {output_dir}")
        
        if len(data) > chunk_size:
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¿å­˜
            num_chunks = (len(data) + chunk_size - 1) // chunk_size
            for i in tqdm(range(num_chunks), desc="ãƒãƒ£ãƒ³ã‚¯ä¿å­˜"):
                start_idx = i * chunk_size
                end_idx = min((i + 1) * chunk_size, len(data))
                chunk_data = data[start_idx:end_idx]
                
                if format == "jsonl":
                    filename = f"data_chunk_{i:04d}.jsonl"
                    if compress:
                        filename += ".gz"
                        with gzip.open(output_path / filename, 'wt', encoding='utf-8') as f:
                            for item in chunk_data:
                                f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    else:
                        with open(output_path / filename, 'w', encoding='utf-8') as f:
                            for item in chunk_data:
                                f.write(json.dumps(item, ensure_ascii=False) + '\n')
                
                elif format == "pickle":
                    filename = f"data_chunk_{i:04d}.pkl"
                    if compress:
                        filename += ".gz"
                        with gzip.open(output_path / filename, 'wb') as f:
                            pickle.dump(chunk_data, f)
                    else:
                        with open(output_path / filename, 'wb') as f:
                            pickle.dump(chunk_data, f)
        else:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            if format == "json":
                filename = "data.json"
                if compress:
                    filename += ".gz"
                    with gzip.open(output_path / filename, 'wt', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                else:
                    with open(output_path / filename, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
            
            elif format == "jsonl":
                # ãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸ã®å ´åˆï¼ˆsplitæ¯ã®ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨é…åˆ—ã®å ´åˆã‚’å‡¦ç†
                if isinstance(data, dict):
                    # splitæ¯ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                    for split, split_data in data.items():
                        filename = f"{split}.jsonl"
                        if compress:
                            filename += ".gz"
                            with gzip.open(output_path / filename, 'wt', encoding='utf-8') as f:
                                for item in split_data:
                                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
                        else:
                            with open(output_path / filename, 'w', encoding='utf-8') as f:
                                for item in split_data:
                                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
                else:
                    # ãƒ‡ãƒ¼ã‚¿ãŒé…åˆ—ã®å ´åˆ
                    filename = "data.jsonl"
                    if compress:
                        filename += ".gz"
                        with gzip.open(output_path / filename, 'wt', encoding='utf-8') as f:
                            for item in data:
                                f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    else:
                        with open(output_path / filename, 'w', encoding='utf-8') as f:
                            for item in data:
                                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å®Œäº†: {output_dir}")
    
    @staticmethod
    def load_dataset(dataset_dir: str) -> Tuple[List[Dict], DatasetMetadata]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            dataset_dir: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            (ãƒ‡ãƒ¼ã‚¿, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)ã®ã‚¿ãƒ—ãƒ«
        """
        dataset_path = Path(dataset_dir)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        metadata_path = dataset_path / "metadata.json"
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata_dict = json.load(f)
            metadata = DatasetMetadata(**metadata_dict)
        
        print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­: {dataset_dir}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data = []
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        chunk_files = sorted(dataset_path.glob("data_chunk_*.jsonl*")) + \
                     sorted(dataset_path.glob("data_chunk_*.pkl*"))
        
        if chunk_files:
            # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
            for chunk_file in tqdm(chunk_files, desc="ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿"):
                if chunk_file.suffix == '.gz':
                    if '.jsonl' in chunk_file.name:
                        with gzip.open(chunk_file, 'rt', encoding='utf-8') as f:
                            for line in f:
                                data.append(json.loads(line))
                    elif '.pkl' in chunk_file.name:
                        with gzip.open(chunk_file, 'rb') as f:
                            chunk_data = pickle.load(f)
                            data.extend(chunk_data)
                else:
                    if chunk_file.suffix == '.jsonl':
                        with open(chunk_file, 'r', encoding='utf-8') as f:
                            for line in f:
                                data.append(json.loads(line))
                    elif chunk_file.suffix == '.pkl':
                        with open(chunk_file, 'rb') as f:
                            chunk_data = pickle.load(f)
                            data.extend(chunk_data)
        else:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            data_files = list(dataset_path.glob("data.*"))
            if data_files:
                data_file = data_files[0]
                if data_file.suffix == '.gz':
                    with gzip.open(data_file, 'rt', encoding='utf-8') as f:
                        if '.json' in data_file.name:
                            data = json.load(f)
                        elif '.jsonl' in data_file.name:
                            data = [json.loads(line) for line in f]
                else:
                    with open(data_file, 'r', encoding='utf-8') as f:
                        if data_file.suffix == '.json':
                            data = json.load(f)
                        elif data_file.suffix == '.jsonl':
                            data = [json.loads(line) for line in f]
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(data):,}ä»¶")
        
        return data, metadata


class DatasetLoader:
    """åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, dataset_dir: str):
        """
        Initialize dataset loader
        
        Args:
            dataset_dir: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.dataset_dir = Path(dataset_dir)
        self.metadata = self._load_metadata()
        self._chunk_files = None
        self._current_chunk = None
        self._current_chunk_idx = -1
    
    def _load_metadata(self) -> DatasetMetadata:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        metadata_path = self.dataset_dir / "metadata.json"
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata_dict = json.load(f)
            return DatasetMetadata(**metadata_dict)
    
    def __len__(self) -> int:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’è¿”ã™"""
        return self.metadata.num_tasks
    
    def __iter__(self) -> Iterator[Dict]:
        """ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨"""
        return self.iter_batches(batch_size=1)
    
    def iter_batches(self, batch_size: int = 32, shuffle: bool = False) -> Iterator[List[Dict]]:
        """
        ãƒãƒƒãƒå˜ä½ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        
        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            shuffle: ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã‹
            
        Yields:
            ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿
        """
        # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆå–å¾—
        if self._chunk_files is None:
            self._chunk_files = sorted(
                self.dataset_dir.glob("data_chunk_*.jsonl*")
            ) + sorted(
                self.dataset_dir.glob("data_chunk_*.pkl*")
            )
        
        if not self._chunk_files:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            data, _ = DatasetSerializer.load_dataset(str(self.dataset_dir))
            if shuffle:
                random.shuffle(data)
            
            for i in range(0, len(data), batch_size):
                yield data[i:i + batch_size]
        else:
            # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§èª­ã¿è¾¼ã¿
            batch_buffer = []
            
            for chunk_file in self._chunk_files:
                # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                chunk_data = self._load_chunk(chunk_file)
                
                if shuffle:
                    random.shuffle(chunk_data)
                
                # ãƒãƒƒãƒä½œæˆ
                for item in chunk_data:
                    batch_buffer.append(item)
                    
                    if len(batch_buffer) >= batch_size:
                        yield batch_buffer[:batch_size]
                        batch_buffer = batch_buffer[batch_size:]
            
            # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿
            if batch_buffer:
                yield batch_buffer
    
    def _load_chunk(self, chunk_file: Path) -> List[Dict]:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if chunk_file.suffix == '.gz':
            if '.jsonl' in chunk_file.name:
                with gzip.open(chunk_file, 'rt', encoding='utf-8') as f:
                    return [json.loads(line) for line in f]
            elif '.pkl' in chunk_file.name:
                with gzip.open(chunk_file, 'rb') as f:
                    return pickle.load(f)
        else:
            if chunk_file.suffix == '.jsonl':
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    return [json.loads(line) for line in f]
            elif chunk_file.suffix == '.pkl':
                with open(chunk_file, 'rb') as f:
                    return pickle.load(f)
        
        return []
    
    def get_sample(self, n: int = 10, seed: int = None) -> List[Dict]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
        
        Args:
            n: ã‚µãƒ³ãƒ—ãƒ«æ•°
            seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
            
        Returns:
            ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        """
        if seed is not None:
            random.seed(seed)
        
        samples = []
        for batch in self.iter_batches(batch_size=min(n, 100)):
            samples.extend(batch)
            if len(samples) >= n:
                break
        
        return random.sample(samples, min(n, len(samples)))


# ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
PRESET_DATASETS = {
    "small": {
        "total_tasks": 10000,
        "description": "å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"
    },
    "medium": {
        "total_tasks": 50000,
        "description": "ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆé–‹ç™ºç”¨ï¼‰"
    },
    "large": {
        "total_tasks": 100000,
        "description": "å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæœ¬ç•ªå­¦ç¿’ç”¨ï¼‰"
    },
    "extra_large": {
        "total_tasks": 500000,
        "description": "è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰"
    }
}


def generate_preset_dataset(
    preset: str,
    output_dir: str,
    task_type_distribution: Optional[Dict[str, float]] = None,
    difficulty_distribution: Optional[Dict[TaskDifficulty, float]] = None,
    **kwargs
) -> Tuple[str, DatasetMetadata]:
    """
    ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
    
    Args:
        preset: ãƒ—ãƒªã‚»ãƒƒãƒˆåï¼ˆsmall, medium, large, extra_largeï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        task_type_distribution: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
        difficulty_distribution: é›£æ˜“åº¦åˆ†å¸ƒ
        **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        (å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)ã®ã‚¿ãƒ—ãƒ«
    """
    if preset not in PRESET_DATASETS:
        raise ValueError(f"Unknown preset: {preset}. Available: {list(PRESET_DATASETS.keys())}")
    
    preset_config = PRESET_DATASETS[preset]
    total_tasks = preset_config["total_tasks"]
    
    print(f"ğŸ¯ ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ: {preset}")
    print(f"   {preset_config['description']}")
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–
    generator = LargeScaleDatasetGenerator()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
    data, metadata = generator.generate_large_dataset(
        total_tasks=total_tasks,
        task_type_distribution=task_type_distribution,
        difficulty_distribution=difficulty_distribution,
        **kwargs
    )
    
    # ä¿å­˜
    DatasetSerializer.save_dataset(
        data=data,
        metadata=metadata,
        output_dir=output_dir,
        format="jsonl",
        compress=True
    )
    
    return output_dir, metadata