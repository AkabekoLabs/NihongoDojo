"""
è¨“ç·´ãƒ­ã‚°ã®åˆ†æãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
"""

import json
import numpy as np
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from collections import defaultdict

# japanize_matplotlibã‚’æ¡ä»¶ä»˜ãã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import japanize_matplotlib
    JAPANIZE_AVAILABLE = True
except ImportError:
    JAPANIZE_AVAILABLE = False
    print("Warning: japanize_matplotlib not found. Japanese text in plots may not display correctly.")
    print("Install with: pip install japanize-matplotlib")


def analyze_training_logs(log_filename):
    """å­¦ç¿’ãƒ­ã‚°ã‚’åˆ†æã™ã‚‹é–¢æ•°ï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç”¨ï¼‰"""
    import os
    
    print("ğŸ“Š å­¦ç¿’ãƒ­ã‚°ã®åˆ†æ")
    print("="*50)
    
    # åŸºæœ¬ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
    if os.path.exists(log_filename):
        with open(log_filename, 'r', encoding='utf-8') as f:
            logs = [json.loads(line) for line in f]
        
        print(f"åŸºæœ¬ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(logs)}")
        
        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        rewards = []
        questions = []
        answers = []
        correct_answers = 0
        
        for log in logs:
            if 'rewards' in log and log['rewards']:
                rewards.extend(log['rewards'])
            if 'question' in log:
                questions.append(log['question'])
            if 'answer' in log and 'extracted' in log:
                answers.append((log['answer'], log['extracted']))
                if log['extracted'] and log['extracted'][0] == log['answer']:
                    correct_answers += 1
        
        if rewards:
            print(f"ç·å ±é…¬ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(rewards)}")
            print(f"å¹³å‡å ±é…¬: {np.mean(rewards):.3f}")
            print(f"æœ€å¤§å ±é…¬: {np.max(rewards):.3f}")
            print(f"æœ€å°å ±é…¬: {np.min(rewards):.3f}")
        
        if answers:
            print(f"æ­£ç­”ç‡: {correct_answers}/{len(answers)} ({correct_answers/len(answers)*100:.1f}%)")
        
        # æœ€æ–°ã®10ã‚¨ãƒ³ãƒˆãƒªã‚’è¡¨ç¤º
        print(f"\nğŸ“‹ æœ€æ–°ã®10ã‚¨ãƒ³ãƒˆãƒª:")
        for i, log in enumerate(logs[-10:]):
            print(f"  {i+1}. ã‚¹ãƒ†ãƒƒãƒ—{log['step']}: {log['question'][:50]}...")
            if log['extracted'] and log['extracted'][0]:
                print(f"     â†’ {log['extracted'][0]}")
            else:
                print(f"     â†’ (å›ç­”ãªã—)")
        
    else:
        print("åŸºæœ¬ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    
    # è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
    detailed_log_file = log_filename.replace('.jsonl', '_detailed.jsonl')
    if os.path.exists(detailed_log_file):
        print(f"\nğŸ“Š è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {detailed_log_file}")
        
        with open(detailed_log_file, 'r', encoding='utf-8') as f:
            detailed_logs = [json.loads(line) for line in f]
        
        print(f"è©³ç´°ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(detailed_logs)}")
        
        # ç”Ÿæˆæ•°ã®çµ±è¨ˆ
        if detailed_logs:
            total_generations = sum(len(item['generations']) for log in detailed_logs for item in log['batch_data'])
            avg_generations_per_question = total_generations / sum(len(log['batch_data']) for log in detailed_logs)
            
            print(f"ç·ç”Ÿæˆæ•°: {total_generations}")
            print(f"è³ªå•ã‚ãŸã‚Šå¹³å‡ç”Ÿæˆæ•°: {avg_generations_per_question:.1f}")
            
            # æœ€é«˜å ±é…¬ã¨æœ€ä½å ±é…¬ã®ä¾‹ã‚’è¡¨ç¤º
            all_generations = []
            for log in detailed_logs:
                for item in log['batch_data']:
                    for gen in item['generations']:
                        all_generations.append({
                            'question': item['question'],
                            'expected': item['expected_answer'],
                            'completion': gen['completion'],
                            'extracted': gen['extracted_answer'],
                            'reward': gen['reward']
                        })
            
            if all_generations:
                # æœ€é«˜å ±é…¬ã®ä¾‹
                best_gen = max(all_generations, key=lambda x: x['reward'])
                print(f"\nğŸ† æœ€é«˜å ±é…¬ã®ä¾‹ (å ±é…¬: {best_gen['reward']:.3f}):")
                print(f"è³ªå•: {best_gen['question']}")
                print(f"æœŸå¾…: {best_gen['expected']}")
                print(f"æŠ½å‡º: {best_gen['extracted']}")
                
                # æœ€ä½å ±é…¬ã®ä¾‹
                worst_gen = min(all_generations, key=lambda x: x['reward'])
                print(f"\nâŒ æœ€ä½å ±é…¬ã®ä¾‹ (å ±é…¬: {worst_gen['reward']:.3f}):")
                print(f"è³ªå•: {worst_gen['question']}")
                print(f"æœŸå¾…: {worst_gen['expected']}")
                print(f"æŠ½å‡º: {worst_gen['extracted']}")
    
    else:
        print("\nè©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")


class TrainingLogAnalyzer:
    """è¨“ç·´ãƒ­ã‚°ã‚’åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, log_filename):
        """
        Args:
            log_filename: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ.jsonlå½¢å¼ï¼‰
        """
        self.log_filename = log_filename
        self.logs = []
        self._load_logs()
    
    def _load_logs(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(self.log_filename, 'r', encoding='utf-8') as f:
                for line in f:
                    self.logs.append(json.loads(line.strip()))
        except FileNotFoundError:
            print(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.log_filename}")
        except Exception as e:
            print(f"ãƒ­ã‚°èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_accuracy_stats(self):
        """ç²¾åº¦çµ±è¨ˆã‚’è¨ˆç®—"""
        if not self.logs:
            return None
        
        total_entries = len(self.logs)
        correct_answers = sum(1 for log in self.logs 
                            if log.get('extracted', [None])[0] == log.get('answer'))
        
        # æŠ½å‡ºæˆåŠŸç‡
        extraction_success = sum(1 for log in self.logs 
                               if log.get('extracted', [None])[0] is not None)
        
        # å ±é…¬ã®çµ±è¨ˆ
        rewards = [log['rewards'][0] for log in self.logs if log.get('rewards')]
        
        return {
            'total_entries': total_entries,
            'correct_answers': correct_answers,
            'accuracy': correct_answers / total_entries * 100 if total_entries > 0 else 0,
            'extraction_success_rate': extraction_success / total_entries * 100 if total_entries > 0 else 0,
            'average_reward': np.mean(rewards) if rewards else 0,
            'max_reward': np.max(rewards) if rewards else 0,
            'min_reward': np.min(rewards) if rewards else 0,
            'std_reward': np.std(rewards) if rewards else 0
        }
    
    def get_question_type_analysis(self):
        """è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†æ"""
        question_types = defaultdict(list)
        
        for log in self.logs:
            question = log.get('question', '')
            # è³ªå•ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            if 'èª­ã¿æ–¹' in question:
                q_type = 'èª­ã¿æ–¹'
            elif 'æ¼¢å­—ã§æ›¸ã' in question:
                q_type = 'æ¼¢å­—æ›¸ã'
            else:
                q_type = 'ãã®ä»–'
            
            is_correct = log.get('extracted', [None])[0] == log.get('answer')
            question_types[q_type].append({
                'correct': is_correct,
                'reward': log['rewards'][0] if log.get('rewards') else 0
            })
        
        # çµ±è¨ˆã‚’è¨ˆç®—
        stats = {}
        for q_type, results in question_types.items():
            if results:
                correct_count = sum(1 for r in results if r['correct'])
                stats[q_type] = {
                    'count': len(results),
                    'accuracy': correct_count / len(results) * 100,
                    'avg_reward': np.mean([r['reward'] for r in results])
                }
        
        return stats
    
    def plot_training_progress(self, window_size=50):
        """è¨“ç·´ã®é€²æ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        if not self.logs:
            print("ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—
        rewards = [log['rewards'][0] for log in self.logs if log.get('rewards')]
        steps = list(range(len(rewards)))
        
        # ç§»å‹•å¹³å‡
        if len(rewards) >= window_size:
            moving_avg = pd.Series(rewards).rolling(window=window_size).mean()
        else:
            moving_avg = rewards
        
        # ãƒ—ãƒ­ãƒƒãƒˆ
        plt.figure(figsize=(12, 6))
        
        # å ±é…¬ã®æ¨ç§»
        plt.subplot(1, 2, 1)
        plt.plot(steps, rewards, alpha=0.3, label='ç”Ÿã®å ±é…¬')
        if len(rewards) >= window_size:
            plt.plot(steps, moving_avg, label=f'{window_size}ã‚¹ãƒ†ãƒƒãƒ—ç§»å‹•å¹³å‡')
        plt.xlabel('ã‚¹ãƒ†ãƒƒãƒ—')
        plt.ylabel('å ±é…¬')
        plt.title('å ±é…¬ã®æ¨ç§»')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ­£ç­”ç‡ã®æ¨ç§»
        plt.subplot(1, 2, 2)
        accuracies = []
        for i in range(0, len(self.logs), window_size):
            batch = self.logs[i:i+window_size]
            correct = sum(1 for log in batch 
                        if log.get('extracted', [None])[0] == log.get('answer'))
            accuracy = correct / len(batch) * 100 if batch else 0
            accuracies.append(accuracy)
        
        acc_steps = list(range(0, len(self.logs), window_size))
        plt.plot(acc_steps, accuracies, marker='o')
        plt.xlabel('ã‚¹ãƒ†ãƒƒãƒ—')
        plt.ylabel('æ­£ç­”ç‡ (%)')
        plt.title(f'{window_size}ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®æ­£ç­”ç‡')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def get_error_analysis(self, num_samples=10):
        """ã‚¨ãƒ©ãƒ¼åˆ†æï¼ˆé–“é•ãˆãŸä¾‹ã‚’æŠ½å‡ºï¼‰"""
        errors = []
        
        for log in self.logs:
            extracted = log.get('extracted', [None])[0]
            answer = log.get('answer')
            
            if extracted != answer:
                errors.append({
                    'step': log.get('step'),
                    'question': log.get('question'),
                    'expected': answer,
                    'got': extracted,
                    'response': log.get('responses', [''])[0][:200] + '...' if log.get('responses') else '',
                    'reward': log.get('rewards', [0])[0]
                })
        
        # å ±é…¬ãŒä½ã„é †ã«ã‚½ãƒ¼ãƒˆ
        errors.sort(key=lambda x: x['reward'])
        
        return errors[:num_samples]
    
    def print_summary(self):
        """ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("=" * 80)
        print("è¨“ç·´ãƒ­ã‚°åˆ†æã‚µãƒãƒªãƒ¼")
        print("=" * 80)
        
        # åŸºæœ¬çµ±è¨ˆ
        stats = self.get_accuracy_stats()
        if stats:
            print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
            print(f"  ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {stats['total_entries']}")
            print(f"  æ­£ç­”æ•°: {stats['correct_answers']}")
            print(f"  æ­£ç­”ç‡: {stats['accuracy']:.2f}%")
            print(f"  æŠ½å‡ºæˆåŠŸç‡: {stats['extraction_success_rate']:.2f}%")
            print(f"  å¹³å‡å ±é…¬: {stats['average_reward']:.3f}")
            print(f"  æœ€å¤§å ±é…¬: {stats['max_reward']:.3f}")
            print(f"  æœ€å°å ±é…¬: {stats['min_reward']:.3f}")
        
        # è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ
        type_stats = self.get_question_type_analysis()
        if type_stats:
            print(f"\nğŸ“ è³ªå•ã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ:")
            for q_type, stat in type_stats.items():
                print(f"  {q_type}:")
                print(f"    - ä»¶æ•°: {stat['count']}")
                print(f"    - æ­£ç­”ç‡: {stat['accuracy']:.2f}%")
                print(f"    - å¹³å‡å ±é…¬: {stat['avg_reward']:.3f}")
        
        # ã‚¨ãƒ©ãƒ¼ä¾‹
        errors = self.get_error_analysis(5)
        if errors:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ä¾‹ï¼ˆå ±é…¬ãŒä½ã„é †ï¼‰:")
            for i, error in enumerate(errors, 1):
                print(f"\n  {i}. ã‚¹ãƒ†ãƒƒãƒ— {error['step']}:")
                print(f"     è³ªå•: {error['question']}")
                print(f"     æœŸå¾…: {error['expected']}")
                print(f"     å–å¾—: {error['got']}")
                print(f"     å ±é…¬: {error['reward']:.3f}")


def analyze_training_logs(log_filename):
    """è¨“ç·´ãƒ­ã‚°ã‚’åˆ†æã™ã‚‹ä¾¿åˆ©é–¢æ•°"""
    analyzer = TrainingLogAnalyzer(log_filename)
    analyzer.print_summary()
    analyzer.plot_training_progress()
    return analyzer