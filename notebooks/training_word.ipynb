{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oxpuos5j-KD"
      },
      "source": [
        "# èªé †ä¸¦ã³æ›¿ãˆã‚¿ã‚¹ã‚¯GRPOå­¦ç¿’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpyO388Oj-KF"
      },
      "source": [
        "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dcVpzNEj-KF"
      },
      "outputs": [],
      "source": [
        "# GPUç’°å¢ƒã®ç¢ºèª\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=== GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯ ===\")\n",
        "print(f\"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPUå: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(\"âœ… GPUç’°å¢ƒãŒæ­£å¸¸ã«æ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼\")\n",
        "else:\n",
        "    print(\"âŒ GPUãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ï¼\")\n",
        "    print(\"ä¸Šè¨˜ã®æ‰‹é †ã§GPUã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚\")\n",
        "    print(\"ãã®å¾Œã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "# Colabç’°å¢ƒã‹ãƒã‚§ãƒƒã‚¯\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    print(f\"\\nGoogle Colab GPU: {os.environ['COLAB_GPU']}\")\n",
        "elif 'COLAB_' in \"\".join(os.environ.keys()):\n",
        "    print(\"\\nGoogle Colabç’°å¢ƒã§ã™ã€‚GPUã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚\")\n",
        "else:\n",
        "    print(\"\\nãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bknR4kx0j-KG"
      },
      "source": [
        "*   unsloth 2025.7.3\n",
        "*   unsloth-zoo 2025.7.4\n",
        "\n",
        "ã®çµ„ã¿åˆã‚ã›ã§å‹•ä½œã€‚\n",
        "\n",
        "https://github.com/unslothai/unsloth/issues/2983"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBf8VhQQj-KG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth==2025.7.3 unsloth-zoo==2025.7.4 vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth==2025.7.3 unsloth-zoo==2025.7.4 vllm==0.8.5.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTPnhXv7j-KG"
      },
      "outputs": [],
      "source": [
        "#@title Colabè¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth==2025.7.3, unsloth-zoo==2025.7.4 vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth==2025.7.3 vllm==0.8.5.post1\n",
        "    # Qwen3_(4B)_GRPO.ipynbã¨åŒã˜è¨­å®š\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth-zoo==2025.7.4\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLMã¯numpyã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãŸã‚Colabã‚’å£Šã™\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6cnOkUoj-KG"
      },
      "source": [
        "## 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N0u2RyGj-KG"
      },
      "outputs": [],
      "source": [
        "# GPUç’°å¢ƒãŒç¢ºèªã§ããŸå ´åˆã®ã¿å®Ÿè¡Œ\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"GPUãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã€‚ä¸Šè¨˜ã®æ‰‹é †ã§GPUã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "max_seq_length = 2048\n",
        "lora_rank = 32\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,\n",
        "    fast_inference = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.7,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = lora_rank*2,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRAXplezj-KH"
      },
      "source": [
        "## 3.nihongo-dojoã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUPDqrlHj-KH"
      },
      "outputs": [],
      "source": [
        "# Google Colabç’°å¢ƒã§ã®æº–å‚™\n",
        "%cd /content\n",
        "!git clone https://github.com/AkabekoLabs/nihongo-dojo\n",
        "%cd /content/nihongo-dojo/\n",
        "!pip install -e .\n",
        "!pip install japanize-matplotlib scikit-learn\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/nihongo-dojo')\n",
        "\n",
        "# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ (é‡è¦)\n",
        "import importlib\n",
        "importlib.invalidate_caches()\n",
        "\n",
        "# ã“ã‚Œã§å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import nihongo_dojo\n",
        "importlib.reload(nihongo_dojo)"
      ],
      "metadata": {
        "id": "oNZdyTZnwTWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZUzZ0Hoj-KH"
      },
      "outputs": [],
      "source": [
        "import sys, importlib\n",
        "module_path = \"/content/nihongo-dojo\"\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "import nihongo_dojo\n",
        "importlib.reload(nihongo_dojo)\n",
        "from nihongo_dojo.colab import TrainingLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQQV1UDj-KH"
      },
      "source": [
        "## 4. ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pKYyBfDj-KH"
      },
      "outputs": [],
      "source": [
        "# ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¨­å®š\n",
        "system_prompt = \"ã‚ãªãŸã¯è¦ªåˆ‡ã§è³¢ã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚\"\n",
        "\n",
        "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] }}{% endif %}\n",
        "\n",
        "{% for message in messages %}{% if message['role'] == 'user' %}\n",
        "User: {{ message['content'] }}\n",
        "\n",
        "{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] }}{% endif %}{% endfor %}\"\"\"\n",
        "\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90ArqXej-KH"
      },
      "source": [
        "## 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StNu2n8Vj-KH"
      },
      "outputs": [],
      "source": [
        "# èªé †ä¸¦ã³æ›¿ãˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ\n",
        "import os\n",
        "if not os.path.exists(\"./datasets/nihongo-dojo-word-order/\"):\n",
        "    %cd /content\n",
        "    !python nihongo-dojo/scripts/generate_datasets.py --tasks WORD_ORDER --custom-size 2000 --output-format jsonl --output-dir ./datasets\n",
        "else:\n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ—¢ã«ç”Ÿæˆæ¸ˆã¿ã§ã™\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3YW7y2Ej-KH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "\n",
        "dataset_path = './datasets/nihongo-dojo-word_order/'\n",
        "\n",
        "# all.jsonlãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
        "import os\n",
        "if os.path.exists(os.path.join(dataset_path, 'all.jsonl')):\n",
        "    # ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•\n",
        "    import json\n",
        "    data = []\n",
        "    with open(os.path.join(dataset_path, 'all.jsonl'), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    dataset = Dataset.from_list(data)\n",
        "elif os.path.exists(os.path.join(dataset_path, 'train.jsonl')):\n",
        "    # train.jsonlã‚’è©¦ã™\n",
        "    import json\n",
        "    data = []\n",
        "    with open(os.path.join(dataset_path, 'train.jsonl'), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    dataset = Dataset.from_list(data)\n",
        "else:\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
        "    raise FileNotFoundError(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_path}\")\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}\")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¾‹ã‚’è¡¨ç¤º\n",
        "print(\"\\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¾‹:\")\n",
        "for i in range(min(3, len(dataset))):\n",
        "    print(f\"\\nä¾‹{i+1}:\")\n",
        "    print(f\"  å•é¡Œ: {dataset[i]['instruction']}{dataset[i]['input']}\")\n",
        "    print(f\"  ç­”ãˆ: {dataset[i]['answer']}\")\n",
        "    print(f\"  èª¬æ˜: {dataset[i]['think'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLpYmxSYj-KH"
      },
      "outputs": [],
      "source": [
        "# ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›\n",
        "formatted_data = []\n",
        "for item in dataset:\n",
        "    question = item['instruction'] + item['input']\n",
        "    answer = item['answer']\n",
        "    think = item['think']\n",
        "    solution = f\"{reasoning_start}\\n{think}\\n{reasoning_end}\\n{solution_start}{answer}{solution_end}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "        {\"role\": \"assistant\", \"content\": solution}\n",
        "    ]\n",
        "\n",
        "    formatted_data.append({\n",
        "        \"Messages\": messages,\n",
        "        \"problem\": question,\n",
        "        \"solution\": solution,\n",
        "        \"answer\": answer,\n",
        "    })\n",
        "\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "print(f\"ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(dataset)}å€‹\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6rMpajlj-KI"
      },
      "source": [
        "## 6. SFTã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå­¦ç¿’ï¼‰\n",
        "\n",
        "ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆå•é¡Œã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã«ã€`<reasoning></reasoning><answer></answer>`ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ãŒã§ãã‚‹ã‚ˆã†ã«SFTã§å­¦ç¿’ã•ã›ã¾ã™ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExuiUaGPj-KI"
      },
      "outputs": [],
      "source": [
        "# çŸ­ã„ä¾‹ã®ã¿ã‚’é¸æŠ\n",
        "dataset = dataset.map(lambda x: {\"N\": len(tokenizer.apply_chat_template(x[\"Messages\"]))})\n",
        "pre_train_dataset = dataset.filter(lambda x: x[\"N\"] <= max_seq_length/2).select(range(min(50, len(dataset))))\n",
        "pre_train_dataset = pre_train_dataset.map(lambda x: {\"text\": tokenizer.apply_chat_template(x[\"Messages\"], tokenize=False)})\n",
        "\n",
        "print(f\"äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(pre_train_dataset)}å€‹\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtGYJOxKj-KI"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = pre_train_dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå­¦ç¿’ã‚’é–‹å§‹...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Cv6aL3j-KI"
      },
      "source": [
        "## 7. ãƒ­ã‚°é–¢é€£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvwwe_aGj-KI"
      },
      "outputs": [],
      "source": [
        "from nihongo_dojo.colab import TrainingLogger\n",
        "\n",
        "# ãƒ­ã‚°ç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆè©³ç´°ãƒ­ã‚°ã‚‚æœ‰åŠ¹åŒ–ï¼‰\n",
        "# ã‚¿ã‚¹ã‚¯åã‚’æŒ‡å®šã—ã¦ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š\n",
        "logger = TrainingLogger(log_dir=\"./logs\", task_name=\"word\", enable_detailed_logging=True)\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰\n",
        "global TRAINING_LOGS, REWARD_LOGS, ACCURACY_STATS\n",
        "TRAINING_LOGS = logger.training_logs\n",
        "REWARD_LOGS = logger.reward_logs\n",
        "ACCURACY_STATS = {\n",
        "    'correct_format': [],\n",
        "    'correct_answer': [],\n",
        "    'partial_answer': [],\n",
        "    'wrong_answer': [],\n",
        "    'no_answer': []\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IynClTaKj-KI"
      },
      "source": [
        "## 8. æ”¹è‰¯ç‰ˆGRPOå ±é…¬é–¢æ•°ã‚’å®šç¾©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UTXEnABj-KI"
      },
      "outputs": [],
      "source": [
        "# æ—¢å­˜ã®å ±é…¬é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ­ã‚°åˆ†æã«åŸºã¥ãã€éƒ¨åˆ†ç‚¹ã‚’è¿½åŠ ï¼‰\n",
        "from nihongo_dojo.reward import JapaneseTaskRewardFunctions\n",
        "import re\n",
        "import difflib\n",
        "\n",
        "# åŸºæœ¬å ±é…¬é–¢æ•°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
        "base_reward_functions = JapaneseTaskRewardFunctions(\n",
        "    reasoning_start=reasoning_start,\n",
        "    reasoning_end=reasoning_end,\n",
        "    solution_start=solution_start,\n",
        "    solution_end=solution_end,\n",
        "    eos_token=tokenizer.eos_token\n",
        ")\n",
        "\n",
        "# èªé †ã‚¿ã‚¹ã‚¯ç”¨ã®æ”¹è‰¯ç‰ˆå ±é…¬é–¢æ•°ã‚’å®šç¾©\n",
        "def improved_check_word_order(prompts=None, completions=None, completion_ids=None, answer=None, **kwargs):\n",
        "    \"\"\"èªé †ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸå ±é…¬é–¢æ•°ï¼ˆéƒ¨åˆ†ç‚¹å¯¾å¿œï¼‰\"\"\"\n",
        "    # åŸºæœ¬çš„ãªç­”ãˆæŠ½å‡º\n",
        "    if prompts is not None and completions is not None:\n",
        "        responses = completions\n",
        "    else:\n",
        "        responses = kwargs.get('completions', [])\n",
        "\n",
        "    # ç­”ãˆã®å‡¦ç†\n",
        "    if answer and isinstance(answer, list) and len(answer) > 0 and isinstance(answer[0], str) and '<answer>' in answer[0]:\n",
        "        extracted_answers = []\n",
        "        for ans in answer:\n",
        "            match = re.search(r'<answer>(.+?)</answer>', ans, re.DOTALL)\n",
        "            if match:\n",
        "                extracted_answers.append(match.group(1).strip())\n",
        "            else:\n",
        "                extracted_answers.append(ans)\n",
        "        answer = extracted_answers\n",
        "\n",
        "    if not isinstance(answer, list):\n",
        "        answer = [answer] * len(responses)\n",
        "\n",
        "    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŠ½å‡º\n",
        "    match_format = re.compile(\n",
        "        rf\"{reasoning_end}.*?\"\n",
        "        rf\"{solution_start}(.+?){solution_end}\"\n",
        "        rf\".*$\",\n",
        "        flags=re.MULTILINE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    extracted_responses = []\n",
        "    for r in responses:\n",
        "        if isinstance(r, str):\n",
        "            text = r\n",
        "        elif isinstance(r, list) and len(r) > 0:\n",
        "            text = r[0].get(\"content\", \"\") if isinstance(r[0], dict) else str(r[0])\n",
        "        else:\n",
        "            text = \"\"\n",
        "\n",
        "        match = match_format.search(text)\n",
        "        if match:\n",
        "            extracted_responses.append(match.group(1).strip())\n",
        "        else:\n",
        "            extracted_responses.append(None)\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼\n",
        "        if guess is None:\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "\n",
        "        # å®Œå…¨ä¸€è‡´\n",
        "        if guess == true_answer:\n",
        "            scores.append(2.0)\n",
        "            continue\n",
        "\n",
        "        # åŒç¾©èªãƒ»è¨€ã„æ›ãˆãƒã‚§ãƒƒã‚¯\n",
        "        synonyms = {\n",
        "            \"ä¸å¯æ¬ ã§ã™\": \"æ¬ ã‹ã›ã¾ã›ã‚“\",\n",
        "            \"æ¬ ã‹ã›ã¾ã›ã‚“\": \"ä¸å¯æ¬ ã§ã™\",\n",
        "            \"å¿…è¦ã§ã™\": \"å¿…è¦ãŒã‚ã‚Šã¾ã™\",\n",
        "            \"å¿…è¦ãŒã‚ã‚Šã¾ã™\": \"å¿…è¦ã§ã™\",\n",
        "        }\n",
        "\n",
        "        # åŒç¾©èªã«ã‚ˆã‚‹ç½®æ›ã‚’è©¦ã™\n",
        "        modified_guess = guess\n",
        "        for orig, syn in synonyms.items():\n",
        "            if orig in true_answer and syn in guess:\n",
        "                modified_guess = guess.replace(syn, orig)\n",
        "                break\n",
        "\n",
        "        if modified_guess == true_answer:\n",
        "            scores.append(1.5)  # åŒç¾©èªä½¿ç”¨\n",
        "            continue\n",
        "\n",
        "        # åŠ©è©ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆã«ã¯ vs ã«ï¼‰\n",
        "        if true_answer.replace(\"ã«ã¯\", \"ã«\") == guess.replace(\"ã«ã¯\", \"ã«\"):\n",
        "            scores.append(1.0)  # åŠ©è©ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³\n",
        "            continue\n",
        "\n",
        "        # ä¸¦åˆ—æ§‹é€ ã®é †åºï¼ˆA ã‚‚ B ã‚‚ï¼‰\n",
        "        if \"ã‚‚\" in guess and \"ã‚‚\" in true_answer:\n",
        "            # ä¸¦åˆ—è¦ç´ ã‚’æŠ½å‡ºã—ã¦æ¯”è¼ƒ\n",
        "            guess_elements = set(re.findall(r'([^ã‚‚]+)ã‚‚', guess))\n",
        "            answer_elements = set(re.findall(r'([^ã‚‚]+)ã‚‚', true_answer))\n",
        "            if guess_elements == answer_elements:\n",
        "                scores.append(1.0)  # é †åºã¯é•ã†ãŒè¦ç´ ã¯åŒã˜\n",
        "                continue\n",
        "\n",
        "        # æ–‡å­—åˆ—é¡ä¼¼åº¦ã«ã‚ˆã‚‹éƒ¨åˆ†ç‚¹\n",
        "        similarity = difflib.SequenceMatcher(None, guess, true_answer).ratio()\n",
        "        if similarity > 0.8:\n",
        "            scores.append(0.5)  # é«˜ã„é¡ä¼¼åº¦\n",
        "        elif similarity > 0.6:\n",
        "            scores.append(0.0)  # ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦\n",
        "        else:\n",
        "            scores.append(-1.0)  # ä½ã„é¡ä¼¼åº¦\n",
        "\n",
        "    return scores\n",
        "\n",
        "print(\"æ”¹è‰¯ç‰ˆå ±é…¬é–¢æ•°:\")\n",
        "print(\"1. match_format_exactly - ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯\")\n",
        "print(\"2. improved_check_word_order - èªé †ãƒã‚§ãƒƒã‚¯ï¼ˆéƒ¨åˆ†ç‚¹å¯¾å¿œï¼‰\")\n",
        "print(\"3. check_reasoning_quality - æ¨è«–å“è³ªãƒã‚§ãƒƒã‚¯\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt-CLCzOj-KI"
      },
      "outputs": [],
      "source": [
        "# nihongo_dojoãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ­ã‚°æ©Ÿèƒ½ã‚’ä½¿ç”¨\n",
        "from nihongo_dojo.colab import LoggingRewardWrapper\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰\n",
        "global PRINTED_TIMES, PRINT_EVERY_STEPS\n",
        "PRINTED_TIMES = 0\n",
        "PRINT_EVERY_STEPS = 5\n",
        "\n",
        "# ãƒ­ã‚°ä»˜ãå ±é…¬é–¢æ•°ã‚’ä½œæˆï¼ˆæ”¹è‰¯ç‰ˆã‚’ä½¿ç”¨ï¼‰\n",
        "check_word_order_with_logging = LoggingRewardWrapper(\n",
        "    reward_func=improved_check_word_order,\n",
        "    logger=logger,\n",
        "    print_every_steps=PRINT_EVERY_STEPS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0zv0unJj-KI"
      },
      "source": [
        "\n",
        "\n",
        "## 9. å¯è¦–åŒ–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCmzNQGPj-KI"
      },
      "outputs": [],
      "source": [
        "# nihongo_dojoãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¯è¦–åŒ–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨\n",
        "from nihongo_dojo.colab import GRPOVisualizationCallback\n",
        "\n",
        "# å¯è¦–åŒ–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œæˆ\n",
        "visualization_callback = GRPOVisualizationCallback(\n",
        "    update_frequency=5,\n",
        "    keep_history_steps=20,\n",
        "    log_filename=logger.log_filename,\n",
        "    logger=logger\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ6QOoZuj-KJ"
      },
      "source": [
        "## 10. GRPOå­¦ç¿’ã®å®Ÿè¡Œï¼ˆæ”¹è‰¯ç‰ˆå ±é…¬é–¢æ•°ä½¿ç”¨ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdT9Kkepj-KJ"
      },
      "outputs": [],
      "source": [
        "# GRPOç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\": [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": x[\"problem\"]},\n",
        "    ],\n",
        "    \"answer\": x[\"solution\"],  # ãƒ•ãƒ«ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä¿æŒï¼ˆå ±é…¬é–¢æ•°å´ã§å®Ÿéš›ã®ç­”ãˆã‚’æŠ½å‡ºï¼‰\n",
        "    \"actual_answer\": x[\"answer\"],  # å®Ÿéš›ã®ç­”ãˆã‚‚ä¿æŒ\n",
        "})\n",
        "\n",
        "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "tokenized = dataset.map(\n",
        "    lambda x: {\"tokens\": tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)},\n",
        "    batched=True,\n",
        ")\n",
        "tokenized = tokenized.map(lambda x: {\"L\": len(x[\"tokens\"])})\n",
        "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
        "print(f\"æœ€å¤§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {maximum_length}\")\n",
        "\n",
        "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
        "print(f\"ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(dataset)}å€‹\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRIiS7Qzj-KJ"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 å¿µã®ãŸã‚\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from vllm import SamplingParams\n",
        "vllm_sampling_params = SamplingParams(\n",
        "    min_p = 0.1,\n",
        "    top_p = 1.0,\n",
        "    top_k = -1,\n",
        "    seed = 3407,\n",
        "    stop = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    vllm_sampling_params = vllm_sampling_params,\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-6,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    num_generations = 4,\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    max_steps = 2000,\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs_word_balanced\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9fZKPCyj-KJ"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        base_reward_functions.match_format_exactly,      # å³æ ¼ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯\n",
        "        check_word_order_with_logging,                  # æ”¹è‰¯ç‰ˆèªé †ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ­ã‚°ä»˜ãï¼‰\n",
        "        base_reward_functions.check_reasoning_quality,  # æ¨è«–å“è³ªãƒã‚§ãƒƒã‚¯\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "    callbacks=[visualization_callback],  # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ \n",
        ")\n",
        "\n",
        "print(\"ğŸ“ èªé †ä¸¦ã³æ›¿ãˆå­¦ç¿’ã®GRPOå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆæ”¹è‰¯ç‰ˆå ±é…¬é–¢æ•°ä½¿ç”¨ï¼‰...\")\n",
        "print(\"ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚°ãƒ©ãƒ•ã¨çµ±è¨ˆæƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™\")\n",
        "print(\"ğŸ’¡ å ±é…¬ãƒãƒ©ãƒ³ã‚¹ãŒæ”¹å–„ã•ã‚Œã€ã‚ˆã‚ŠåŠ¹æœçš„ãªå­¦ç¿’ãŒæœŸå¾…ã§ãã¾ã™\")\n",
        "print(\"-\"*80)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmyAbB_Bj-KJ"
      },
      "source": [
        "\n",
        "\n",
        "## 11. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2wwKjs6j-KJ"
      },
      "outputs": [],
      "source": [
        "# LoRAãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
        "model.save_lora(\"grpo_word_balanced_lora\")\n",
        "print(\"æ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O-c5O0Xj-KJ"
      },
      "outputs": [],
      "source": [
        "# èªé †ä¸¦ã³æ›¿ãˆãƒ†ã‚¹ãƒˆ\n",
        "test_questions = [\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚æ˜¨æ—¥ / æ˜ ç”»ã‚’ / å‹é”ã¨ / è¦‹ã¾ã—ãŸ\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚å­¦æ ¡ã« / æ˜æ—¥ / è¡Œãã¾ã™ / ç§ã¯\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚èª­ã‚“ã§ã„ã¾ã™ / æ¯æ—¥ / æœ¬ã‚’ / å½¼ã¯\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚ãŠã„ã—ã‹ã£ãŸ / ã¨ã¦ã‚‚ / ã‚±ãƒ¼ã‚­ã¯ / æ˜¨æ—¥ã®\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚è¡ŒããŸã„ã§ã™ / ã„ã¤ã‹ / æ—¥æœ¬ã« / ç§ã¯\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚çµ‚ã‚ã£ã¦ã‹ã‚‰ / å®¿é¡Œã‚’ / ã—ã¾ã™ / æˆæ¥­ãŒ\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚å¥½ãã§ã™ / éŸ³æ¥½ã‚’ / èãã®ãŒ / ç§ã¯\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚è¡Œãã¾ã—ãŸ / å…¬åœ’ã« / æ•£æ­©ã« / æœ\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚è²·ã„ã¾ã—ãŸ / æ–°ã—ã„ / æ˜¨æ—¥ / æœ¬ã‚’\",\n",
        "    \"æ¬¡ã®å˜èªã‚’æ­£ã—ã„é †ç•ªã«ä¸¦ã³æ›¿ãˆã¦æ–‡ã‚’ä½œã£ã¦ãã ã•ã„ã€‚å‹‰å¼·ã—ã¦ã„ã¾ã™ / ãŸã‚ã« / è©¦é¨“ã® / ä¸€ç”Ÿæ‡¸å‘½\",\n",
        "]\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ—¾ èªé †ä¸¦ã³æ›¿ãˆãƒ†ã‚¹ãƒˆï¼ˆæ”¹è‰¯ç‰ˆãƒ¢ãƒ‡ãƒ«ï¼‰\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "\n",
        "    output = model.fast_generate(\n",
        "        text,\n",
        "        sampling_params=sampling_params,\n",
        "        lora_request=model.load_lora(\"grpo_word_balanced_lora\"),\n",
        "    )[0].outputs[0].text\n",
        "\n",
        "    print(f\"\\n{i}. å•é¡Œ: {question}\")\n",
        "    print(f\"   å¿œç­”: {output}\")\n",
        "    print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6FZTz2Qj-KJ"
      },
      "source": [
        "## 12. å­¦ç¿’ãƒ­ã‚°ã®åˆ†æ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQH7JRm9j-KJ"
      },
      "outputs": [],
      "source": [
        "# nihongo_dojoãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å¯è¦–åŒ–é–¢æ•°ã‚’ä½¿ç”¨\n",
        "from nihongo_dojo.colab.visualization import plot_training_history\n",
        "\n",
        "# å­¦ç¿’å±¥æ­´ã‚’å¯è¦–åŒ–\n",
        "plot_training_history(logger.history_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru0v2ieZj-KJ"
      },
      "outputs": [],
      "source": [
        "# æ‹¡å¼µãƒ­ã‚°åˆ†æ\n",
        "from nihongo_dojo.colab import analyze_training_logs\n",
        "\n",
        "print(\"\\nğŸ“Š æ‹¡å¼µãƒ­ã‚°åˆ†æã‚’å®Ÿè¡Œä¸­...\")\n",
        "analyze_training_logs(logger.log_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYC5l7Bsj-KJ"
      },
      "source": [
        "## 13. æ”¹å–„ã®åŠ¹æœã‚’æ¯”è¼ƒ\n",
        "\n",
        "æ”¹è‰¯ç‰ˆã®å ±é…¬é–¢æ•°ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¾ã™ï¼š\n",
        "\n",
        "1. **å ±é…¬åˆ†å¸ƒã®æ”¹å–„**: ã‚ˆã‚Šé©åˆ‡ãªéƒ¨åˆ†ç‚¹ã«ã‚ˆã‚‹å­¦ç¿’ã®ä¿ƒé€²\n",
        "2. **èªé †è©•ä¾¡ã®è©³ç´°åŒ–**: å˜èªã®ä½ç½®ã€åŠ©è©ã®ä¿æŒã€æ–‡æœ«ã®æ­£ç¢ºæ€§ã‚’è©•ä¾¡\n",
        "3. **å“è³ªã®å‘ä¸Š**: èªé †ã«é–¢ã™ã‚‹æ–‡æ³•èª¬æ˜ã®è³ªãŒå‘ä¸Š\n",
        "4. **ã‚¨ãƒ©ãƒ¼ã®æ¸›å°‘**: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼ã¨å®Œå…¨ã«èª¤ã£ãŸç­”ãˆã®å‰Šæ¸›"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnGs7Hbuj-KK"
      },
      "outputs": [],
      "source": [
        "# å ±é…¬åˆ†å¸ƒã®åˆ†æ\n",
        "print(\"\\nğŸ“Š å ±é…¬åˆ†å¸ƒã®åˆ†æ\")\n",
        "print(\"æ”¹è‰¯ç‰ˆã®å ±é…¬é–¢æ•°ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®å¤‰åŒ–ãŒæœŸå¾…ã•ã‚Œã¾ã™ï¼š\")\n",
        "print(\"\\nã€å¾“æ¥ç‰ˆã€‘\")\n",
        "print(\"- 2.0 (å®Œå…¨æ­£è§£): 97.7%\")\n",
        "print(\"- -2.0 (ä¸æ­£è§£): 2.3%\")\n",
        "print(\"- ãƒã‚¤ãƒŠãƒªãƒ¼ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã§ä¸­é–“è©•ä¾¡ãªã—\")\n",
        "print(\"\\nã€æ”¹è‰¯ç‰ˆï¼ˆæœŸå¾…å€¤ï¼‰ã€‘\")\n",
        "print(\"- 2.0 (å®Œå…¨æ­£è§£): 40-50%\")\n",
        "print(\"- 1.5 (åŒç¾©èªä½¿ç”¨): 10-15%\")\n",
        "print(\"- 1.0 (åŠ©è©ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³/ä¸¦åˆ—é †åº): 15-20%\")\n",
        "print(\"- 0.5 (é«˜ã„é¡ä¼¼åº¦): 10-15%\")\n",
        "print(\"- 0.0 (ä¸­ç¨‹åº¦ã®é¡ä¼¼åº¦): 5-10%\")\n",
        "print(\"- -1.0 (ä½ã„é¡ä¼¼åº¦): 3-5%\")\n",
        "print(\"- -2.0 (ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼): <2%\")\n",
        "print(\"\\nç‰¹ã«ã€åŒç¾©èªã‚„åŠ©è©ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®è©•ä¾¡ã«ã‚ˆã‚Šã€ã‚ˆã‚ŠæŸ”è»Ÿã§è‡ªç„¶ãªæ—¥æœ¬èªå­¦ç¿’ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufxvq18Xj-KK"
      },
      "source": [
        "## 14. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}