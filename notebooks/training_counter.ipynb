{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 助数詞（カウンター）タスクGRPO学習ノートブック（改良版）\n",
    "\n",
    "training_counter.ipynbのログ分析結果に基づいて、報酬バランスを改善したバージョンです。\n",
    "\n",
    "## 主な改善点\n",
    "1. **報酬の厳格化**: 数字が正しくても助数詞が間違っている場合の部分点を削減\n",
    "2. **カテゴリーベース評価**: 動物系（匹/羽/頭）などカテゴリー内での混同を考慮\n",
    "3. **文法説明の強化**: 助数詞の使い分けに関する説明を重視\n",
    "4. **数字の正確性チェック**: 漢数字・全角数字の正規化対応\n",
    "\n",
    "## ログ分析結果\n",
    "- 完全正解(3.0): 43.5%\n",
    "- 部分点(0.2-2.5): 50.0%（寛大すぎる）\n",
    "- 間違い(<0): 6.5%\n",
    "- 平均報酬: 1.45（高すぎる）\n",
    "\n",
    "改良版では、より厳格な評価により正しい助数詞の使用を促進します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU環境の確認\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"=== GPU環境チェック ===\")\n",
    "print(f\"CUDA利用可能: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUメモリ: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"✅ GPU環境が正常に検出されました！\")\n",
    "else:\n",
    "    print(\"❌ GPUが検出されません！\")\n",
    "    print(\"上記の手順でGPUを有効にしてください。\")\n",
    "    print(\"その後、ランタイムを再起動してこのセルを再実行してください。\")\n",
    "\n",
    "# Colab環境かチェック\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    print(f\"\\nGoogle Colab GPU: {os.environ['COLAB_GPU']}\")\n",
    "elif 'COLAB_' in \"\".join(os.environ.keys()):\n",
    "    print(\"\\nGoogle Colab環境です。GPUを有効にしてください。\")\n",
    "else:\n",
    "    print(\"\\nローカル環境で実行中\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   unsloth 2025.7.3\n",
    "*   unsloth-zoo 2025.7.4\n",
    "\n",
    "の組み合わせで動作。\n",
    "\n",
    "https://github.com/unslothai/unsloth/issues/2983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth==2025.7.3 unsloth-zoo==2025.7.4 vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth==2025.7.3 unsloth-zoo==2025.7.4 vllm==0.8.5.post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab追加インストール { display-mode: \"form\" }\n",
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth==2025.7.3, unsloth-zoo==2025.7.4 vllm\n",
    "else:\n",
    "    !pip install --no-deps unsloth==2025.7.3 vllm==0.8.5.post1\n",
    "    # Qwen3_(4B)_GRPO.ipynbと同じ設定\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth-zoo==2025.7.4\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLMはnumpyを再インストールするためColabを壊す\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU環境が確認できた場合のみ実行\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPUが検出されません。上記の手順でGPUを有効にしてください。\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(\"モデルをロード中...\")\n",
    "max_seq_length = 2048\n",
    "lora_rank = 32\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False,\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = lora_rank*2,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.nihongo-dojoのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab環境での準備\n",
    "%cd /content\n",
    "!unzip nihongo-dojo.zip\n",
    "!pip install japanize-matplotlib scikit-learn\n",
    "%cd /content/nihongo-dojo/\n",
    "!pip install -e .\n",
    "%cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "module_path = \"/content/nihongo-dojo\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import nihongo_dojo\n",
    "importlib.reload(nihongo_dojo)\n",
    "from nihongo_dojo.colab import TrainingLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. チャットテンプレートの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャットテンプレートを設定\n",
    "system_prompt = \"あなたは親切で賢いアシスタントです。日本語の助数詞を正しく使って数を数えてください。\"\n",
    "\n",
    "# デフォルト設定\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] }}{% endif %}\n",
    "\n",
    "{% for message in messages %}{% if message['role'] == 'user' %}\n",
    "User: {{ message['content'] }}\n",
    "\n",
    "{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 助数詞データセット生成\n",
    "import os\n",
    "if not os.path.exists(\"./datasets/nihongo-dojo-counter/\"):\n",
    "    %cd /content\n",
    "    !python nihongo-dojo/scripts/generate_datasets.py --tasks COUNTER_WORD --custom-size 2000 --output-format jsonl --output-dir ./datasets\n",
    "else:\n",
    "    print(\"データセットは既に生成済みです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(\"データセットを読み込み中...\")\n",
    "\n",
    "dataset_path = './datasets/nihongo-dojo-counter_word/'\n",
    "\n",
    "# all.jsonlファイルが存在するか確認\n",
    "import os\n",
    "if os.path.exists(os.path.join(dataset_path, 'all.jsonl')):\n",
    "    # 直接ファイルを読み込む方法\n",
    "    import json\n",
    "    data = []\n",
    "    with open(os.path.join(dataset_path, 'all.jsonl'), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    dataset = Dataset.from_list(data)\n",
    "elif os.path.exists(os.path.join(dataset_path, 'train.jsonl')):\n",
    "    # train.jsonlを試す\n",
    "    import json\n",
    "    data = []\n",
    "    with open(os.path.join(dataset_path, 'train.jsonl'), 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    dataset = Dataset.from_list(data)\n",
    "else:\n",
    "    # データセットが見つからない場合のエラーメッセージ\n",
    "    raise FileNotFoundError(f\"データセットが見つかりません: {dataset_path}\")\n",
    "\n",
    "print(f\"データセットサイズ: {len(dataset)}\")\n",
    "\n",
    "# データセットの例を表示\n",
    "print(\"\\nデータセットの例:\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print(f\"\\n例{i+1}:\")\n",
    "    print(f\"  問題: {dataset[i]['instruction']}{dataset[i]['input']}\")\n",
    "    print(f\"  答え: {dataset[i]['answer']}\")\n",
    "    print(f\"  説明: {dataset[i]['think'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォーマット変換\n",
    "formatted_data = []\n",
    "for item in dataset:\n",
    "    question = item['instruction'] + item['input']\n",
    "    answer = item['answer']\n",
    "    think = item['think']\n",
    "    solution = f\"{reasoning_start}\\n{think}\\n{reasoning_end}\\n{solution_start}{answer}{solution_end}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": solution}\n",
    "    ]\n",
    "\n",
    "    formatted_data.append({\n",
    "        \"Messages\": messages,\n",
    "        \"problem\": question,\n",
    "        \"solution\": solution,\n",
    "        \"answer\": answer,\n",
    "    })\n",
    "\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "print(f\"フォーマット済みデータセット: {len(dataset)}個\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SFTによる事前学習（フォーマット学習）\n",
    "\n",
    "コールドスタート問題を解消するために、`<reasoning></reasoning><answer></answer>`のフォーマットで出力ができるようにSFTで学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 短い例のみを選択\n",
    "dataset = dataset.map(lambda x: {\"N\": len(tokenizer.apply_chat_template(x[\"Messages\"]))})\n",
    "pre_train_dataset = dataset.filter(lambda x: x[\"N\"] <= max_seq_length/2).select(range(min(50, len(dataset))))\n",
    "pre_train_dataset = pre_train_dataset.map(lambda x: {\"text\": tokenizer.apply_chat_template(x[\"Messages\"], tokenize=False)})\n",
    "\n",
    "print(f\"事前学習データセット: {len(pre_train_dataset)}個\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = pre_train_dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"フォーマット学習を開始...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ログ関連"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nihongo_dojo.colab import TrainingLogger\n",
    "\n",
    "# ログ管理インスタンスを作成（詳細ログも有効化）\n",
    "# タスク名を指定してログファイル名を設定\n",
    "logger = TrainingLogger(log_dir=\"./logs\", task_name=\"counter\", enable_detailed_logging=True)\n",
    "\n",
    "# グローバル変数（互換性のため）\n",
    "global TRAINING_LOGS, REWARD_LOGS, ACCURACY_STATS\n",
    "TRAINING_LOGS = logger.training_logs\n",
    "REWARD_LOGS = logger.reward_logs\n",
    "ACCURACY_STATS = {\n",
    "    'correct_format': [],\n",
    "    'correct_answer': [],\n",
    "    'partial_answer': [],\n",
    "    'wrong_answer': [],\n",
    "    'no_answer': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 改良版GRPO報酬関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改良版の報酬関数をインポート\n",
    "from nihongo_dojo.reward import CounterRewardFunctions\n",
    "\n",
    "# 改良版報酬関数インスタンスを作成\n",
    "counter_reward_functions = CounterRewardFunctions(\n",
    "    reasoning_start=reasoning_start,\n",
    "    reasoning_end=reasoning_end,\n",
    "    solution_start=solution_start,\n",
    "    solution_end=solution_end,\n",
    "    eos_token=tokenizer.eos_token\n",
    ")\n",
    "\n",
    "# バランスの取れた報酬関数を取得\n",
    "balanced_rewards = counter_reward_functions.get_balanced_reward_functions()\n",
    "\n",
    "print(\"改良版報酬関数:\")\n",
    "print(\"1. strict_format_check - 厳格なフォーマットチェック\")\n",
    "print(\"2. balanced_check_counter - バランスの取れた助数詞チェック\")\n",
    "print(\"3. enhanced_counter_quality - 強化された品質チェック\")\n",
    "print(\"4. check_number_accuracy - 数字正確性チェック\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nihongo_dojoライブラリのログ機能を使用\n",
    "from nihongo_dojo.colab import LoggingRewardWrapper\n",
    "\n",
    "# グローバル変数（後方互換性のため）\n",
    "global PRINTED_TIMES, PRINT_EVERY_STEPS\n",
    "PRINTED_TIMES = 0\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "# ログ付き報酬関数を作成（改良版を使用）\n",
    "check_counter_with_logging = LoggingRewardWrapper(\n",
    "    reward_func=counter_reward_functions.balanced_check_counter,\n",
    "    logger=logger,\n",
    "    print_every_steps=PRINT_EVERY_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 可視化コールバックの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nihongo_dojoライブラリの可視化コールバックを使用\n",
    "from nihongo_dojo.colab import GRPOVisualizationCallback\n",
    "\n",
    "# 可視化コールバックを作成\n",
    "visualization_callback = GRPOVisualizationCallback(\n",
    "    update_frequency=5,\n",
    "    keep_history_steps=20,\n",
    "    log_filename=logger.log_filename,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. GRPO学習の実行（改良版報酬関数使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO用にフォーマット\n",
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": x[\"problem\"]},\n",
    "    ],\n",
    "    \"answer\": x[\"solution\"],  # フルソリューションを保持（報酬関数側で実際の答えを抽出）\n",
    "    \"actual_answer\": x[\"answer\"],  # 実際の答えも保持\n",
    "})\n",
    "\n",
    "# プロンプト長でフィルタリング\n",
    "tokenized = dataset.map(\n",
    "    lambda x: {\"tokens\": tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)},\n",
    "    batched=True,\n",
    ")\n",
    "tokenized = tokenized.map(lambda x: {\"L\": len(x[\"tokens\"])})\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(f\"最大プロンプト長: {maximum_length}\")\n",
    "\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "print(f\"フィルタ後のデータセット: {len(dataset)}個\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 念のため\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    num_generations = 4,\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_steps = 2000,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs_counter_balanced\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        counter_reward_functions.strict_format_check,      # 厳格なフォーマットチェック\n",
    "        check_counter_with_logging,                        # バランスの取れた助数詞チェック（ログ付き）\n",
    "        counter_reward_functions.enhanced_counter_quality, # 強化された品質チェック\n",
    "        counter_reward_functions.check_number_accuracy,    # 数字正確性チェック\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "    callbacks=[visualization_callback],  # ビジュアライゼーションコールバックを追加\n",
    ")\n",
    "\n",
    "print(\"🌸 助数詞学習のGRPO学習を開始します（改良版報酬関数使用）...\")\n",
    "print(\"📊 リアルタイムでグラフと統計情報が表示されます\")\n",
    "print(\"💡 報酬バランスが改善され、より効果的な学習が期待できます\")\n",
    "print(\"-\"*80)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAモデルを保存\n",
    "model.save_lora(\"grpo_counter_balanced_lora\")\n",
    "print(\"改良版モデルを保存しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 助数詞テスト\n",
    "test_questions = [\n",
    "    \"鳥が5いる。\",\n",
    "    \"本を3読んだ。\",\n",
    "    \"車が2ある。\",\n",
    "    \"手紙を4書いた。\",\n",
    "    \"猫が6いる。\",\n",
    "    \"建物が7ある。\",\n",
    "    \"紙を8使った。\",\n",
    "    \"時計が9ある。\",\n",
    "    \"靴を2買った。\",\n",
    "    \"会議室は3にあります。\",\n",
    "]\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🗾 助数詞テスト（改良版モデル）\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    output = model.fast_generate(\n",
    "        text,\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=model.load_lora(\"grpo_counter_balanced_lora\"),\n",
    "    )[0].outputs[0].text\n",
    "\n",
    "    print(f\"\\n{i}. 問題: {question}\")\n",
    "    print(f\"   応答: {output}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 学習ログの分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nihongo_dojoライブラリの可視化関数を使用\n",
    "from nihongo_dojo.colab.visualization import plot_training_history\n",
    "\n",
    "# 学習履歴を可視化\n",
    "plot_training_history(logger.history_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拡張ログ分析\n",
    "from nihongo_dojo.colab import analyze_training_logs\n",
    "\n",
    "print(\"\\n📊 拡張ログ分析を実行中...\")\n",
    "analyze_training_logs(logger.log_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 改善の効果を比較\n",
    "\n",
    "改良版の報酬関数により、以下の改善が期待されます：\n",
    "\n",
    "1. **報酬分布の改善**: 平均報酬1.45→より厳格な評価による適正化\n",
    "2. **カテゴリー認識**: 動物系（匹/羽/頭）などカテゴリー内での使い分け学習\n",
    "3. **品質の向上**: 助数詞の使い分けに関する説明の充実\n",
    "4. **エラーの減少**: 数字は正しいが助数詞が間違っているケースの削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 報酬分布の分析\n",
    "print(\"\\n📊 報酬分布の分析\")\n",
    "print(\"改良版の報酬関数により、以下の変化が期待されます：\")\n",
    "print(\"\\n【従来版】\")\n",
    "print(\"- 3.0 (完全正解): 43.5%\")\n",
    "print(\"- 0.5 (数字正しい・助数詞違い): 45%（寛大すぎる）\")\n",
    "print(\"- その他部分点: 5%\")\n",
    "print(\"- -1.5 (不正解): 6.5%\")\n",
    "print(\"- 平均報酬: 1.45\")\n",
    "print(\"\\n【改良版（期待値）】\")\n",
    "print(\"- 2.0 (完全正解): 30-40%\")\n",
    "print(\"- 0.2 (同カテゴリー内違い): 15-25%\")\n",
    "print(\"- -0.5 (関連カテゴリー違い): 20-30%\")\n",
    "print(\"- -1.0 (無関係カテゴリー): 10-15%\")\n",
    "print(\"- -2.0 (数字も違う): 5-10%\")\n",
    "print(\"- -3.0 (フォーマットエラー): <2%\")\n",
    "print(\"\\n特に、匹と羽の混同など、よくある間違いパターンに対してより適切な学習シグナルを提供します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. モデルの保存（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なモデルをHuggingFaceに保存（オプション）\n",
    "# model.push_to_hub_merged(\"username/grpo-counter-balanced\", tokenizer, save_method=\"lora\")\n",
    "print(\"モデルの保存準備が完了しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}