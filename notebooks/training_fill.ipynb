{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygtcyyyyj3te"
      },
      "source": [
        "# 助詞穴埋めタスクGRPO学習ノートブック\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thZG_Iaxj3tf"
      },
      "source": [
        "## 1. 環境セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoJ0cRfkj3tf"
      },
      "outputs": [],
      "source": [
        "# GPU環境の確認\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=== GPU環境チェック ===\")\n",
        "print(f\"CUDA利用可能: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPUメモリ: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(\"✅ GPU環境が正常に検出されました！\")\n",
        "else:\n",
        "    print(\"❌ GPUが検出されません！\")\n",
        "    print(\"上記の手順でGPUを有効にしてください。\")\n",
        "    print(\"その後、ランタイムを再起動してこのセルを再実行してください。\")\n",
        "\n",
        "# Colab環境かチェック\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    print(f\"\\nGoogle Colab GPU: {os.environ['COLAB_GPU']}\")\n",
        "elif 'COLAB_' in \"\".join(os.environ.keys()):\n",
        "    print(\"\\nGoogle Colab環境です。GPUを有効にしてください。\")\n",
        "else:\n",
        "    print(\"\\nローカル環境で実行中\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX-zSHioj3tg"
      },
      "source": [
        "*   unsloth 2025.7.3\n",
        "*   unsloth-zoo 2025.7.4\n",
        "\n",
        "の組み合わせで動作。\n",
        "\n",
        "https://github.com/unslothai/unsloth/issues/2983"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IexBZ68mj3tg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth==2025.7.3 unsloth-zoo==2025.7.4 vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth==2025.7.3 unsloth-zoo==2025.7.4 vllm==0.8.5.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiXjOJRNj3tg"
      },
      "outputs": [],
      "source": [
        "#@title Colab追加インストール { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth==2025.7.3, unsloth-zoo==2025.7.4 vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth==2025.7.3 vllm==0.8.5.post1\n",
        "    # Qwen3_(4B)_GRPO.ipynbと同じ設定\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth-zoo==2025.7.4\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLMはnumpyを再インストールするためColabを壊す\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKVcFHmDj3tg"
      },
      "source": [
        "## 2. モデルのロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLriBS-bj3tg"
      },
      "outputs": [],
      "source": [
        "# GPU環境が確認できた場合のみ実行\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"GPUが検出されません。上記の手順でGPUを有効にしてください。\")\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(\"モデルをロード中...\")\n",
        "max_seq_length = 2048\n",
        "lora_rank = 32\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,\n",
        "    fast_inference = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.7,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = lora_rank*2,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5d0yaICj3th"
      },
      "source": [
        "## 3.nihongo-dojoのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odJVujuAj3th"
      },
      "outputs": [],
      "source": [
        "# Google Colab環境での準備\n",
        "%cd /content\n",
        "!git clone https://github.com/AkabekoLabs/nihongo-dojo\n",
        "%cd /content/nihongo-dojo/\n",
        "!pip install -e .\n",
        "!pip install japanize-matplotlib scikit-learn\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USof_yllj3th"
      },
      "outputs": [],
      "source": [
        "import sys, importlib\n",
        "module_path = \"/content/nihongo-dojo\"\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "import nihongo_dojo\n",
        "importlib.reload(nihongo_dojo)\n",
        "from nihongo_dojo.colab import TrainingLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYqGKnjLj3th"
      },
      "source": [
        "## 4. チャットテンプレートの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4YbtGpfj3th"
      },
      "outputs": [],
      "source": [
        "# チャットテンプレートを設定\n",
        "system_prompt = \"あなたは親切で賢いアシスタントです。日本語の助詞を正しく選んで文を完成させてください。\"\n",
        "\n",
        "# デフォルト設定\n",
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] }}{% endif %}\n",
        "\n",
        "{% for message in messages %}{% if message['role'] == 'user' %}\n",
        "User: {{ message['content'] }}\n",
        "\n",
        "{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] }}{% endif %}{% endfor %}\"\"\"\n",
        "\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOXX2faKj3th"
      },
      "source": [
        "## 5. データセットの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfjjgoIPj3th"
      },
      "outputs": [],
      "source": [
        "# 助詞穴埋めデータセット生成\n",
        "import os\n",
        "if not os.path.exists(\"./datasets/nihongo-dojo-particle-fill/\"):\n",
        "    %cd /content\n",
        "    !python nihongo-dojo/scripts/generate_datasets.py --tasks PARTICLE_FILL --custom-size 2000 --output-format jsonl --output-dir ./datasets\n",
        "else:\n",
        "    print(\"データセットは既に生成済みです\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-19gJ4_j3th"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "print(\"データセットを読み込み中...\")\n",
        "\n",
        "dataset_path = './datasets/nihongo-dojo-particle_fill/'\n",
        "\n",
        "# all.jsonlファイルが存在するか確認\n",
        "import os\n",
        "if os.path.exists(os.path.join(dataset_path, 'all.jsonl')):\n",
        "    # 直接ファイルを読み込む方法\n",
        "    import json\n",
        "    data = []\n",
        "    with open(os.path.join(dataset_path, 'all.jsonl'), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    dataset = Dataset.from_list(data)\n",
        "elif os.path.exists(os.path.join(dataset_path, 'train.jsonl')):\n",
        "    # train.jsonlを試す\n",
        "    import json\n",
        "    data = []\n",
        "    with open(os.path.join(dataset_path, 'train.jsonl'), 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    dataset = Dataset.from_list(data)\n",
        "else:\n",
        "    # データセットが見つからない場合のエラーメッセージ\n",
        "    raise FileNotFoundError(f\"データセットが見つかりません: {dataset_path}\")\n",
        "\n",
        "print(f\"データセットサイズ: {len(dataset)}\")\n",
        "\n",
        "# データセットの例を表示\n",
        "print(\"\\nデータセットの例:\")\n",
        "for i in range(min(3, len(dataset))):\n",
        "    print(f\"\\n例{i+1}:\")\n",
        "    print(f\"  問題: {dataset[i]['instruction']}{dataset[i]['input']}\")\n",
        "    print(f\"  答え: {dataset[i]['answer']}\")\n",
        "    print(f\"  説明: {dataset[i]['think'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dGm442Uj3th"
      },
      "outputs": [],
      "source": [
        "# フォーマット変換\n",
        "formatted_data = []\n",
        "for item in dataset:\n",
        "    question = item['instruction'] + item['input']\n",
        "    answer = item['answer']\n",
        "    think = item['think']\n",
        "    solution = f\"{reasoning_start}\\n{think}\\n{reasoning_end}\\n{solution_start}{answer}{solution_end}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "        {\"role\": \"assistant\", \"content\": solution}\n",
        "    ]\n",
        "\n",
        "    formatted_data.append({\n",
        "        \"Messages\": messages,\n",
        "        \"problem\": question,\n",
        "        \"solution\": solution,\n",
        "        \"answer\": answer,\n",
        "    })\n",
        "\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "print(f\"フォーマット済みデータセット: {len(dataset)}個\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8qFnlqjj3th"
      },
      "source": [
        "## 6. SFTによる事前学習（フォーマット学習）\n",
        "\n",
        "コールドスタート問題を解消するために、`<reasoning></reasoning><answer></answer>`のフォーマットで出力ができるようにSFTで学習させます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsN2uyRtj3th"
      },
      "outputs": [],
      "source": [
        "# 短い例のみを選択\n",
        "dataset = dataset.map(lambda x: {\"N\": len(tokenizer.apply_chat_template(x[\"Messages\"]))})\n",
        "pre_train_dataset = dataset.filter(lambda x: x[\"N\"] <= max_seq_length/2).select(range(min(50, len(dataset))))\n",
        "pre_train_dataset = pre_train_dataset.map(lambda x: {\"text\": tokenizer.apply_chat_template(x[\"Messages\"], tokenize=False)})\n",
        "\n",
        "print(f\"事前学習データセット: {len(pre_train_dataset)}個\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qwAFrScj3ti"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = pre_train_dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"フォーマット学習を開始...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIOmhVXMj3ti"
      },
      "source": [
        "## 7. ログ関連"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFOIKfLKj3ti"
      },
      "outputs": [],
      "source": [
        "from nihongo_dojo.colab import TrainingLogger\n",
        "\n",
        "# ログ管理インスタンスを作成（詳細ログも有効化）\n",
        "# タスク名を指定してログファイル名を設定\n",
        "logger = TrainingLogger(log_dir=\"./logs\", task_name=\"fill\", enable_detailed_logging=True)\n",
        "\n",
        "# グローバル変数（互換性のため）\n",
        "global TRAINING_LOGS, REWARD_LOGS, ACCURACY_STATS\n",
        "TRAINING_LOGS = logger.training_logs\n",
        "REWARD_LOGS = logger.reward_logs\n",
        "ACCURACY_STATS = {\n",
        "    'correct_format': [],\n",
        "    'correct_answer': [],\n",
        "    'partial_answer': [],\n",
        "    'wrong_answer': [],\n",
        "    'no_answer': []\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Q59KS0j3ti"
      },
      "source": [
        "## 8. 改良版GRPO報酬関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TNHtScej3ti"
      },
      "outputs": [],
      "source": [
        "# 改良版の報酬関数を定義（データの偏りに対応）\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# 基本報酬関数をインポート\n",
        "from nihongo_dojo.reward import JapaneseTaskRewardFunctions\n",
        "\n",
        "# 基本報酬関数インスタンスを作成\n",
        "base_reward_functions = JapaneseTaskRewardFunctions(\n",
        "    reasoning_start=reasoning_start,\n",
        "    reasoning_end=reasoning_end,\n",
        "    solution_start=solution_start,\n",
        "    solution_end=solution_end,\n",
        "    eos_token=tokenizer.eos_token\n",
        ")\n",
        "\n",
        "# 答えの分布を記録\n",
        "answer_distribution = Counter()\n",
        "\n",
        "def improved_check_particle(prompts=None, completions=None, completion_ids=None, answer=None, **kwargs):\n",
        "    \"\"\"助詞穴埋めタスクに特化した改良版報酬関数\"\"\"\n",
        "    global answer_distribution\n",
        "\n",
        "    # 基本的な答え抽出\n",
        "    if prompts is not None and completions is not None:\n",
        "        responses = completions\n",
        "    else:\n",
        "        responses = kwargs.get('completions', [])\n",
        "\n",
        "    # 答えの処理\n",
        "    if answer and isinstance(answer, list) and len(answer) > 0 and isinstance(answer[0], str) and '<answer>' in answer[0]:\n",
        "        extracted_answers = []\n",
        "        for ans in answer:\n",
        "            match = re.search(r'<answer>(.+?)</answer>', ans, re.DOTALL)\n",
        "            if match:\n",
        "                extracted_answers.append(match.group(1).strip())\n",
        "            else:\n",
        "                extracted_answers.append(ans)\n",
        "        answer = extracted_answers\n",
        "\n",
        "    if not isinstance(answer, list):\n",
        "        answer = [answer] * len(responses)\n",
        "\n",
        "    # フォーマット抽出\n",
        "    match_format = re.compile(\n",
        "        rf\"{reasoning_end}.*?\"\n",
        "        rf\"{solution_start}(.+?){solution_end}\"\n",
        "        rf\".*$\",\n",
        "        flags=re.MULTILINE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    extracted_responses = []\n",
        "    for r in responses:\n",
        "        if isinstance(r, str):\n",
        "            text = r\n",
        "        elif isinstance(r, list) and len(r) > 0:\n",
        "            text = r[0].get(\"content\", \"\") if isinstance(r[0], dict) else str(r[0])\n",
        "        else:\n",
        "            text = \"\"\n",
        "\n",
        "        match = match_format.search(text)\n",
        "        if match:\n",
        "            extracted_responses.append(match.group(1).strip())\n",
        "        else:\n",
        "            extracted_responses.append(None)\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    # 頻出助詞のペナルティ係数（「に」が多すぎる問題への対処）\n",
        "    frequency_penalty = {\n",
        "        \"に\": 0.8,  # 頻出のためペナルティ\n",
        "        \"が\": 0.9,\n",
        "        \"を\": 0.9,\n",
        "        \"で\": 1.0,\n",
        "        \"と\": 1.0,\n",
        "        \"へ\": 1.1,  # 少ない助詞にボーナス\n",
        "        \"から\": 1.1,\n",
        "        \"まで\": 1.1,\n",
        "        \"より\": 1.2,\n",
        "    }\n",
        "\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        # 答えの分布を記録\n",
        "        if true_answer:\n",
        "            answer_distribution[true_answer] += 1\n",
        "\n",
        "        # フォーマットエラー\n",
        "        if guess is None:\n",
        "            scores.append(-3.0)\n",
        "            continue\n",
        "\n",
        "        # 頻度に基づくペナルティ/ボーナスを適用\n",
        "        penalty = frequency_penalty.get(true_answer, 1.0)\n",
        "\n",
        "        # 完全一致\n",
        "        if guess == true_answer:\n",
        "            scores.append(2.0 * penalty)\n",
        "            continue\n",
        "\n",
        "        # 助詞のカテゴリー別評価\n",
        "        particle_categories = {\n",
        "            \"格助詞\": [\"が\", \"を\", \"に\", \"で\", \"と\", \"へ\", \"から\", \"まで\", \"より\"],\n",
        "            \"接続助詞\": [\"ば\", \"と\", \"ても\", \"けど\", \"から\", \"ので\", \"のに\"],\n",
        "            \"副助詞\": [\"は\", \"も\", \"こそ\", \"さえ\", \"でも\", \"しか\", \"ばかり\"],\n",
        "            \"終助詞\": [\"か\", \"ね\", \"よ\", \"な\", \"の\"],\n",
        "        }\n",
        "\n",
        "        # カテゴリー内の混同チェック\n",
        "        guess_category = None\n",
        "        answer_category = None\n",
        "\n",
        "        for category, particles in particle_categories.items():\n",
        "            if guess in particles:\n",
        "                guess_category = category\n",
        "            if true_answer in particles:\n",
        "                answer_category = category\n",
        "\n",
        "        if guess_category == answer_category and guess_category is not None:\n",
        "            # 同じカテゴリー内の混同\n",
        "            if guess_category == \"格助詞\":\n",
        "                # 格助詞の混同は特に評価を細かく\n",
        "                if (guess, true_answer) in [(\"に\", \"で\"), (\"で\", \"に\")]:\n",
        "                    scores.append(0.3)  # 場所の「に」と手段の「で」\n",
        "                elif (guess, true_answer) in [(\"が\", \"を\"), (\"を\", \"が\")]:\n",
        "                    scores.append(-0.5)  # 主格と対格の混同は重大\n",
        "                else:\n",
        "                    scores.append(0.0)\n",
        "            else:\n",
        "                scores.append(0.5)  # 他のカテゴリー内混同\n",
        "        else:\n",
        "            # 異なるカテゴリー間の混同\n",
        "            scores.append(-1.5)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def enhanced_particle_reasoning(prompts=None, completions=None, completion_ids=None, answer=None, **kwargs):\n",
        "    \"\"\"助詞選択の理由説明を評価する報酬関数\"\"\"\n",
        "    if prompts is not None and completions is not None:\n",
        "        responses = completions\n",
        "    else:\n",
        "        responses = kwargs.get('completions', [])\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    # 理由説明のパターン\n",
        "    good_patterns = [\n",
        "        r\"対象を表す\",\n",
        "        r\"場所を表す\",\n",
        "        r\"手段を表す\",\n",
        "        r\"方向を表す\",\n",
        "        r\"基準を表す\",\n",
        "        r\"主語を表す\",\n",
        "        r\"目的語を表す\",\n",
        "        r\"格助詞\",\n",
        "        r\"接続助詞\",\n",
        "        r\"副助詞\",\n",
        "    ]\n",
        "\n",
        "    for r in responses:\n",
        "        if isinstance(r, str):\n",
        "            text = r\n",
        "        elif isinstance(r, list) and len(r) > 0:\n",
        "            text = r[0].get(\"content\", \"\") if isinstance(r[0], dict) else str(r[0])\n",
        "        else:\n",
        "            text = \"\"\n",
        "\n",
        "        # reasoning部分を抽出\n",
        "        reasoning_match = re.search(\n",
        "            rf\"{reasoning_start}(.+?){reasoning_end}\",\n",
        "            text,\n",
        "            re.DOTALL\n",
        "        )\n",
        "\n",
        "        if not reasoning_match:\n",
        "            scores.append(-1.0)\n",
        "            continue\n",
        "\n",
        "        reasoning = reasoning_match.group(1)\n",
        "\n",
        "        # 良い説明のパターンをチェック\n",
        "        pattern_count = sum(1 for pattern in good_patterns if re.search(pattern, reasoning))\n",
        "\n",
        "        if pattern_count >= 2:\n",
        "            scores.append(1.0)\n",
        "        elif pattern_count == 1:\n",
        "            scores.append(0.5)\n",
        "        else:\n",
        "            scores.append(0.0)\n",
        "\n",
        "    return scores\n",
        "\n",
        "print(\"改良版報酬関数:\")\n",
        "print(\"1. base_reward_functions.strict_format_check - 厳格なフォーマットチェック\")\n",
        "print(\"2. improved_check_particle - 改良版助詞チェック（頻度調整付き）\")\n",
        "print(\"3. enhanced_particle_reasoning - 理由説明の評価\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCLzEG-rj3ti"
      },
      "outputs": [],
      "source": [
        "# nihongo_dojoライブラリのログ機能を使用\n",
        "from nihongo_dojo.colab import LoggingRewardWrapper\n",
        "\n",
        "# グローバル変数（後方互換性のため）\n",
        "global PRINTED_TIMES, PRINT_EVERY_STEPS\n",
        "PRINTED_TIMES = 0\n",
        "PRINT_EVERY_STEPS = 5\n",
        "\n",
        "# ログ付き報酬関数を作成（改良版を使用）\n",
        "check_particle_with_logging = LoggingRewardWrapper(\n",
        "    reward_func=improved_check_particle,\n",
        "    logger=logger,\n",
        "    print_every_steps=PRINT_EVERY_STEPS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YW3Wejdj3ti"
      },
      "source": [
        "## 9. 可視化コールバックの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD-w6v-Fj3ti"
      },
      "outputs": [],
      "source": [
        "# nihongo_dojoライブラリの可視化コールバックを使用\n",
        "from nihongo_dojo.colab import GRPOVisualizationCallback\n",
        "\n",
        "# 可視化コールバックを作成\n",
        "visualization_callback = GRPOVisualizationCallback(\n",
        "    update_frequency=5,\n",
        "    keep_history_steps=20,\n",
        "    log_filename=logger.log_filename,\n",
        "    logger=logger\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFTG-b2vj3ti"
      },
      "source": [
        "## 10. GRPO学習の実行（改良版報酬関数使用）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQSj3AgOj3ti"
      },
      "outputs": [],
      "source": [
        "# GRPO用にフォーマット\n",
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\": [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": x[\"problem\"]},\n",
        "    ],\n",
        "    \"answer\": x[\"solution\"],  # フルソリューションを保持（報酬関数側で実際の答えを抽出）\n",
        "    \"actual_answer\": x[\"answer\"],  # 実際の答えも保持\n",
        "})\n",
        "\n",
        "# プロンプト長でフィルタリング\n",
        "tokenized = dataset.map(\n",
        "    lambda x: {\"tokens\": tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)},\n",
        "    batched=True,\n",
        ")\n",
        "tokenized = tokenized.map(lambda x: {\"L\": len(x[\"tokens\"])})\n",
        "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
        "print(f\"最大プロンプト長: {maximum_length}\")\n",
        "\n",
        "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
        "print(f\"フィルタ後のデータセット: {len(dataset)}個\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgEqSPACj3ti"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 念のため\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from vllm import SamplingParams\n",
        "vllm_sampling_params = SamplingParams(\n",
        "    min_p = 0.1,\n",
        "    top_p = 1.0,\n",
        "    top_k = -1,\n",
        "    seed = 3407,\n",
        "    stop = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    vllm_sampling_params = vllm_sampling_params,\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-6,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    num_generations = 4,\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    max_steps = 2000,\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs_particle_balanced\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmkrJaccj3ti"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        base_reward_functions.match_format_exactly,      # 厳格なフォーマットチェック\n",
        "        check_particle_with_logging,                    # 改良版助詞チェック（ログ付き）\n",
        "        enhanced_particle_reasoning,                    # 理由説明の評価\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "    callbacks=[visualization_callback],  # ビジュアライゼーションコールバックを追加\n",
        ")\n",
        "\n",
        "print(\"🌸 助詞穴埋め学習のGRPO学習を開始します（改良版報酬関数使用）...\")\n",
        "print(\"📊 リアルタイムでグラフと統計情報が表示されます\")\n",
        "print(\"💡 データの偏りに対応した報酬関数で効果的な学習を実現します\")\n",
        "print(\"-\"*80)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zj8vpLMj3tj"
      },
      "source": [
        "## 11. モデルの評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATCQL_HYj3tj"
      },
      "outputs": [],
      "source": [
        "# LoRAモデルを保存\n",
        "model.save_lora(\"grpo_particle_balanced_lora\")\n",
        "print(\"改良版モデルを保存しました\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6yNEvTBj3tj"
      },
      "outputs": [],
      "source": [
        "# 助詞穴埋めテスト\n",
        "test_questions = [\n",
        "    \"私（　）学校に行きます。\",\n",
        "    \"本（　）読みます。\",\n",
        "    \"友達（　）遊びます。\",\n",
        "    \"電車（　）東京へ行きます。\",\n",
        "    \"ペン（　）字を書きます。\",\n",
        "    \"公園（　）走ります。\",\n",
        "    \"母（　）プレゼントをもらいました。\",\n",
        "    \"日本（　）アメリカは遠いです。\",\n",
        "    \"朝ご飯（　）食べましたか？\",\n",
        "    \"これ（　）私の本です。\",\n",
        "]\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🗾 助詞穴埋めテスト（改良版モデル）\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "\n",
        "    output = model.fast_generate(\n",
        "        text,\n",
        "        sampling_params=sampling_params,\n",
        "        lora_request=model.load_lora(\"grpo_particle_balanced_lora\"),\n",
        "    )[0].outputs[0].text\n",
        "\n",
        "    print(f\"\\n{i}. 問題: {question}\")\n",
        "    print(f\"   応答: {output}\")\n",
        "    print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. モデルの保存（オプション）"
      ],
      "metadata": {
        "id": "3mDdvywsGa6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 最終的なモデルをHuggingFaceに保存（オプション）\n",
        "# model.push_to_hub_merged(\"username/grpo-counter-balanced\", tokenizer, save_method=\"lora\")\n",
        "print(\"モデルの保存準備が完了しました。\")"
      ],
      "metadata": {
        "id": "hK8IUmLSGbDn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}