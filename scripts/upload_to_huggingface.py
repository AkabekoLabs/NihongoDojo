"""
Script to upload Nihongo DoJo datasets to Hugging Face Hub
"""

import os
import sys
import argparse
import json
import gzip
from pathlib import Path
from typing import List, Dict, Optional
from datasets import Dataset, DatasetDict, load_dataset
from huggingface_hub import HfApi, create_repo

from nihongo_dojo.data.huggingface_dataset_builder import HuggingFaceDatasetBuilder


def generate_dataset_card(dataset_name: str, dataset: DatasetDict, source_path: str) -> str:
    """
    æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
    """
    total_samples = sum(len(dataset[split]) for split in dataset)
    splits_info = "\n".join([f"- {split}: {len(dataset[split]):,} ã‚µãƒ³ãƒ—ãƒ«" for split in dataset])
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
    sample_data = ""
    if 'train' in dataset and len(dataset['train']) > 0:
        sample = dataset['train'][0]
        # sampleãŒè¾æ›¸å‹ã§ãªã„å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è¾æ›¸ã¨ã—ã¦å–å¾—
        if hasattr(sample, 'to_dict'):
            sample_dict = sample.to_dict()
        elif isinstance(sample, dict):
            sample_dict = sample
        else:
            # Datasetã®Rowã‹ã‚‰è¾æ›¸ã‚’ä½œæˆ
            sample_dict = {key: sample[key] for key in dataset['train'].column_names}
        sample_data = f"""
## ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿

```json
{json.dumps(sample_dict, ensure_ascii=False, indent=2)[:1000]}...
```
"""
    
    return f"""---
language:
- ja
license: mit
task_categories:
- text-generation
- question-answering
tags:
- japanese
- education
- grpo
pretty_name: {dataset_name}
size_categories:
- {get_size_category(total_samples)}
---

# {dataset_name}

ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€Nihongo DoJoãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ

{splits_info}
- ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples:,}

## ã‚½ãƒ¼ã‚¹

ç”Ÿæˆå…ƒ: `{source_path}`

{sample_data}

## ä½¿ç”¨æ–¹æ³•

```python
from datasets import load_dataset

dataset = load_dataset("{dataset_name}")
print(dataset['train'][0])
```

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License
"""


def get_size_category(num_samples: int) -> str:
    """ã‚µãƒ³ãƒ—ãƒ«æ•°ã‹ã‚‰ã‚µã‚¤ã‚ºã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®š"""
    if num_samples < 1000:
        return "n<1K"
    elif num_samples < 10000:
        return "1K<n<10K"
    elif num_samples < 100000:
        return "10K<n<100K"
    elif num_samples < 1000000:
        return "100K<n<1M"
    else:
        return "1M<n<10M"


def generate_kanji_dataset_card(dataset_name: str, grades: List[int], dataset: DatasetDict) -> str:
    """æ¼¢å­—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®ã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰"""
    return f"""---
language:
- ja
license: mit
task_categories:
- text-generation
- question-answering
tags:
- japanese
- kanji
- education
- grpo
pretty_name: {dataset_name}
size_categories:
- {get_size_category(sum(len(dataset[split]) for split in dataset))}
---

# {dataset_name}

å°å­¦æ ¡{min(grades)}å¹´ç”Ÿã‹ã‚‰{max(grades)}å¹´ç”Ÿã¾ã§ã®æ•™è‚²æ¼¢å­—ã‚’åéŒ²ã—ãŸæ¼¢å­—èª­ã¿ç·´ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™ã€‚

## çµ±è¨ˆæƒ…å ±

- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(dataset.get('train', [])):,} ã‚µãƒ³ãƒ—ãƒ«
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(dataset.get('validation', [])):,} ã‚µãƒ³ãƒ—ãƒ«
- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(dataset.get('test', [])):,} ã‚µãƒ³ãƒ—ãƒ«

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License
"""


def load_local_dataset(path: str) -> Dict:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        path: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    
    Returns:
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¾æ›¸
    """
    path = Path(path)
    
    if path.is_file():
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        if path.suffix == '.json':
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {'train': data}
        
        elif path.suffix == '.jsonl' or (path.suffix == '.gz' and path.stem.endswith('.jsonl')):
            data = []
            if path.suffix == '.gz':
                with gzip.open(path, 'rt', encoding='utf-8') as f:
                    for line in f:
                        data.append(json.loads(line))
            else:
                with open(path, 'r', encoding='utf-8') as f:
                    for line in f:
                        data.append(json.loads(line))
            return {'train': data}
        
        elif path.suffix == '.parquet':
            dataset = load_dataset('parquet', data_files=str(path))
            return dataset
        
        elif path.suffix == '.csv':
            dataset = load_dataset('csv', data_files=str(path))
            return dataset
    
    elif path.is_dir():
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ
        dataset_dict = {}
        
        # ã¾ãš data.jsonl.gz ã¾ãŸã¯ data.jsonl ã‚’æ¢ã™ï¼ˆgenerate_datasets.pyã®å‡ºåŠ›å½¢å¼ï¼‰
        data_jsonl_gz = path / "data.jsonl.gz"
        data_jsonl = path / "data.jsonl"
        
        if data_jsonl_gz.exists():
            data = []
            with gzip.open(data_jsonl_gz, 'rt', encoding='utf-8') as f:
                for line in f:
                    data.append(json.loads(line))
            # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            total = len(data)
            train_size = int(total * 0.8)
            val_size = int(total * 0.1)
            dataset_dict['train'] = data[:train_size]
            dataset_dict['validation'] = data[train_size:train_size+val_size]
            dataset_dict['test'] = data[train_size+val_size:]
            return dataset_dict
        elif data_jsonl.exists():
            data = []
            with open(data_jsonl, 'r', encoding='utf-8') as f:
                for line in f:
                    data.append(json.loads(line))
            # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            total = len(data)
            train_size = int(total * 0.8)
            val_size = int(total * 0.1)
            dataset_dict['train'] = data[:train_size]
            dataset_dict['validation'] = data[train_size:train_size+val_size]
            dataset_dict['test'] = data[train_size+val_size:]
            return dataset_dict
        
        # æ¬¡ã« splitåˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        for split in ['train', 'validation', 'test']:
            jsonl_file = path / f"{split}.jsonl"
            jsonl_gz_file = path / f"{split}.jsonl.gz"
            
            if jsonl_gz_file.exists():
                data = []
                with gzip.open(jsonl_gz_file, 'rt', encoding='utf-8') as f:
                    for line in f:
                        data.append(json.loads(line))
                dataset_dict[split] = data
            elif jsonl_file.exists():
                data = []
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        data.append(json.loads(line))
                dataset_dict[split] = data
        
        if dataset_dict:
            return dataset_dict
        
        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        parquet_files = list(path.glob("*.parquet"))
        if parquet_files:
            return load_dataset('parquet', data_dir=str(path))
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        csv_files = list(path.glob("*.csv"))
        if csv_files:
            data_files = {}
            for csv_file in csv_files:
                split = csv_file.stem.split('_')[-1]
                if split in ['train', 'validation', 'test']:
                    data_files[split] = str(csv_file)
            return load_dataset('csv', data_files=data_files)
    
    raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : {path}")


def main():
    parser = argparse.ArgumentParser(
        description="Nihongo DoJo ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
  # generate_datasets.pyã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
  python upload_to_huggingface.py --input-path ./datasets/nihongo-dojo-medium --dataset-name nihongo-dojo-medium
  
  # çµ„ç¹”ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
  python upload_to_huggingface.py --input-path ./datasets/custom.jsonl --dataset-name my-dataset --organization my-org --private
  
  # æ–°è¦ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
  python upload_to_huggingface.py --dataset-name nihongo-kanji --dataset-type kanji --grades 1 2 3
        """
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¥åŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument("--input-path", type=str, help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹")
    
    # Hugging Faceè¨­å®š
    parser.add_argument("--dataset-name", type=str, required=True, help="Hugging Face Hubä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå")
    parser.add_argument("--organization", type=str, default=None, help="çµ„ç¹”åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    parser.add_argument("--private", action="store_true", help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã™ã‚‹")
    parser.add_argument("--token", type=str, default=None, help="HuggingFace APIãƒˆãƒ¼ã‚¯ãƒ³")
    
    # ãƒ¬ã‚¬ã‚·ãƒ¼ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    parser.add_argument("--dataset-type", type=str, choices=["kanji", "grpo", "chat", "all"], help="ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰")
    parser.add_argument("--grades", type=int, nargs="+", help="å¯¾è±¡å­¦å¹´ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰")
    
    args = parser.parse_args()
    
    # Hugging Face API token
    token = args.token or os.environ.get("HF_TOKEN")
    if not token:
        print("è­¦å‘Š: HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`huggingface-cli login`ã§ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“")
    
    # Repository name
    repo_id = f"{args.organization}/{args.dataset_name}" if args.organization else args.dataset_name
    
    # å…¥åŠ›ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆæ¨å¥¨ï¼‰
    if args.input_path:
        print(f"ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­: {args.input_path}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
            dataset_dict = load_local_dataset(args.input_path)
            
            # DatasetDictã«å¤‰æ›
            if isinstance(dataset_dict, dict):
                datasets_to_upload = {}
                for split, data in dataset_dict.items():
                    if isinstance(data, list):
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
                        processed_data = []
                        for item in data:
                            processed_item = item.copy()
                            if 'metadata' in processed_item and isinstance(processed_item['metadata'], dict):
                                processed_item['metadata'] = json.dumps(processed_item['metadata'], ensure_ascii=False)
                            processed_data.append(processed_item)
                        datasets_to_upload[split] = Dataset.from_list(processed_data)
                    else:
                        datasets_to_upload[split] = data
                dataset = DatasetDict(datasets_to_upload)
            else:
                dataset = dataset_dict
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
            for split in dataset:
                print(f"   {split}: {len(dataset[split]):,} ã‚µãƒ³ãƒ—ãƒ«")
            
        except Exception as e:
            import traceback
            print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("\nãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
            print(traceback.format_exc())
            return
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        dataset_card = generate_dataset_card(args.dataset_name, dataset, args.input_path)
    
    # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
    elif args.dataset_type:
        print("âš ï¸  ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ¢ãƒ¼ãƒ‰: æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã€‚--input-pathã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        builder = HuggingFaceDatasetBuilder()
        
        if args.dataset_type == "kanji" or args.dataset_type == "all":
            print(f"æ¼¢å­—èª­ã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­ï¼ˆå­¦å¹´: {args.grades or 'å…¨å­¦å¹´'}ï¼‰...")
            dataset = builder.create_kanji_reading_dataset(
                num_samples_per_grade=1000,
                grades=args.grades or [1, 2, 3, 4, 5, 6],
                include_on_yomi=True,
                include_kun_yomi=True
            )
            dataset_card = generate_kanji_dataset_card(args.dataset_name, args.grades or [1, 2, 3, 4, 5, 6], dataset)
        
        
        elif args.dataset_type == "grpo":
            print("GRPO ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­...")
            dataset = builder.create_grpo_dataset(
                num_groups=1000,
                group_size=4
            )
            dataset_card = generate_dataset_card(args.dataset_name, dataset, "generated")
        
        elif args.dataset_type == "chat":
            print("ãƒãƒ£ãƒƒãƒˆå½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­...")
            dataset = builder.create_chat_format_dataset(
                num_samples=5000
            )
            dataset_card = generate_dataset_card(args.dataset_name, dataset, "generated")
        
        else:  # "all"
            print("å…¨ã‚¿ã‚¤ãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç‰¹å®šã®ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            return
    
    else:
        parser.error("--input-pathã¾ãŸã¯--dataset-typeã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        return
    
    # ãƒªãƒã‚¸ãƒˆãƒªä½œæˆ
    print(f"\nğŸ“¤ ãƒªãƒã‚¸ãƒˆãƒªä½œæˆä¸­: {repo_id}")
    try:
        create_repo(
            repo_id=repo_id,
            token=token,
            private=args.private,
            repo_type="dataset",
            exist_ok=True
        )
    except Exception as e:
        print(f"ãƒªãƒã‚¸ãƒˆãƒªãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    print("ğŸ“¤ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        dataset.push_to_hub(
            repo_id,
            token=token,
            private=args.private,
            commit_message="Upload dataset via Nihongo DoJo"
        )
        
        # READMEã‚’æ›´æ–°
        api = HfApi()
        api.upload_file(
            path_or_fileobj=dataset_card.encode(),
            path_in_repo="README.md",
            repo_id=repo_id,
            repo_type="dataset",
            token=token,
            commit_message="Update dataset card"
        )
        
        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ­£å¸¸ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ: https://huggingface.co/datasets/{repo_id}")
        
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print("ãƒˆãƒ¼ã‚¯ãƒ³ã®æ¨©é™ã‚„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()