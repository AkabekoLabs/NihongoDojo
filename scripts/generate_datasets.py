#!/usr/bin/env python
"""
Nihongo DoJo ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
äº‹å‰ç”Ÿæˆã•ã‚ŒãŸå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
"""

import argparse
import os
import sys
import json
from pathlib import Path
from typing import List, Optional

from nihongo_dojo.data.dataset_builder import create_all_preset_datasets
from nihongo_dojo.data.large_scale_datasets import (
    LargeScaleDatasetGenerator,
    DatasetSerializer,
    generate_preset_dataset
)
from nihongo_dojo.core import TaskDifficulty, TaskType


def main():
    parser = argparse.ArgumentParser(
        description="Nihongo DoJoå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¾‹:
  # å­¦å¹´1,2,3ã®æ¼¢å­—èª­ã¿ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
  python generate_datasets.py --grades 1 2 3 --tasks KANJI_READING
  
  # å…¨ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã§ä¸­ç´šãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
  python generate_datasets.py --preset medium
  
  # ã‚«ã‚¹ã‚¿ãƒ ã‚µã‚¤ã‚ºã§ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
  python generate_datasets.py --custom-size 10000 --tasks KANJI_READING KANJI_WRITING
        """
    )
    
    # åŸºæœ¬è¨­å®š
    parser.add_argument(
        "--preset",
        type=str,
        choices=["small", "medium", "large", "extra_large", "all"],
        help="ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆgrades/tasksã¨ä½µç”¨ä¸å¯ï¼‰"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./datasets",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    parser.add_argument(
        "--output-format",
        type=str,
        choices=["json", "jsonl", "parquet", "csv"],
        default="jsonl",
        help="å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"
    )
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰ã¨ã‚¿ã‚¹ã‚¯æŒ‡å®š
    parser.add_argument(
        "--grades",
        type=int,
        nargs="+",
        choices=[1, 2, 3, 4, 5, 6],
        help="å¯¾è±¡å­¦å¹´ï¼ˆ1-6ï¼‰"
    )
    
    parser.add_argument(
        "--tasks",
        type=str,
        nargs="+",
        help="ç”Ÿæˆã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ï¼ˆä¾‹: KANJI_READING KANJI_WRITINGï¼‰"
    )
    
    # ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--custom-size",
        type=int,
        help="ã‚«ã‚¹ã‚¿ãƒ ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¿ã‚¹ã‚¯æ•°ï¼‰"
    )
    
    parser.add_argument(
        "--task-distribution",
        type=str,
        help="ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®åˆ†å¸ƒï¼ˆä¾‹: basic:0.6,advanced:0.3,cultural:0.1ï¼‰"
    )
    
    parser.add_argument(
        "--difficulty-distribution",
        type=str,
        default="beginner:0.25,intermediate:0.35,advanced:0.25,native:0.15",
        help="é›£æ˜“åº¦ã®åˆ†å¸ƒ"
    )
    
    parser.add_argument(
        "--num-workers",
        type=int,
        help="ä¸¦åˆ—å‡¦ç†ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°"
    )
    
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰"
    )
    
    parser.add_argument(
        "--compress",
        action="store_true",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åœ§ç¸®ã™ã‚‹"
    )
    
    args = parser.parse_args()
    
    # å¼•æ•°ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    if args.preset and (args.grades or args.tasks):
        parser.error("--presetã¨--grades/--tasksã¯åŒæ™‚ã«æŒ‡å®šã§ãã¾ã›ã‚“")
    
    if not args.preset and not args.grades and not args.tasks and not args.custom_size:
        parser.error("--presetã€--grades/--tasksã€ã¾ãŸã¯--custom-sizeã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æº–å‚™
    selected_tasks = None
    if args.tasks:
        selected_tasks = []
        for task_name in args.tasks:
            try:
                task_type = TaskType[task_name]
                selected_tasks.append(task_type)
            except KeyError:
                print(f"è­¦å‘Š: ä¸æ˜ãªã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ— '{task_name}' ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
                print(f"åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {', '.join([t.name for t in TaskType])}")
        
        if not selected_tasks:
            parser.error("æœ‰åŠ¹ãªã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ã‚°ãƒ¬ãƒ¼ãƒ‰ã¨ã‚¿ã‚¹ã‚¯ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
    if args.grades or args.tasks:
        try:
            from nihongo_dojo.data.huggingface_dataset_builder import HuggingFaceDatasetBuilder
        except ImportError:
            from data.huggingface_dataset_builder import HuggingFaceDatasetBuilder
        
        builder = HuggingFaceDatasetBuilder()
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
        output_name = "nihongo-dojo"
        if args.grades:
            output_name += f"-grades{'-'.join(map(str, args.grades))}"
        if args.tasks:
            task_names = '-'.join([t.name.lower() for t in selected_tasks])
            output_name += f"-{task_names}"
        
        output_path = os.path.join(args.output_dir, output_name)
        
        print(f"ğŸ¯ ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ:")
        if args.grades:
            print(f"   å­¦å¹´: {', '.join(map(str, args.grades))}")
        if selected_tasks:
            print(f"   ã‚¿ã‚¹ã‚¯: {', '.join([t.value for t in selected_tasks])}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
        if selected_tasks and len(selected_tasks) == 1 and selected_tasks[0] == TaskType.PARTICLE_FILL:
            # åŠ©è©ç©´åŸ‹ã‚ã‚¿ã‚¹ã‚¯ã®ã¿ã®å ´åˆ
            print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ä¸­: {output_path}")
            dataset = builder.create_grpo_dataset(
                num_groups=args.custom_size // 4 if args.custom_size else 500,
                group_size=4,
                task_types=[TaskType.PARTICLE_FILL]
            )
        elif selected_tasks and (TaskType.KANJI_READING in selected_tasks or TaskType.KANJI_WRITING in selected_tasks):
            # æ¼¢å­—èª­ã¿/æ›¸ãã‚¿ã‚¹ã‚¯ã®å ´åˆ
            dataset = builder.create_kanji_reading_dataset(
                num_samples_per_grade=1000,
                grades=args.grades or [1, 2, 3, 4, 5, 6],
                include_on_yomi=True,
                include_kun_yomi=True,
                include_writing=(TaskType.KANJI_WRITING in selected_tasks)
            )
        else:
            # ãã®ä»–ã®ã‚¿ã‚¹ã‚¯ã¾ãŸã¯æ··åˆã‚¿ã‚¹ã‚¯ã®å ´åˆ
            dataset = builder.create_grpo_dataset(
                num_groups=args.custom_size // 4 if args.custom_size else 1000,
                group_size=4,
                task_types=selected_tasks
            )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜
        if args.output_format == "json":
            # JSONå½¢å¼ã§ä¿å­˜
            all_data = []
            
            # DatasetDictã®å ´åˆã¨Datasetã®å ´åˆã‚’å‡¦ç†
            from datasets import DatasetDict
            if isinstance(dataset, DatasetDict):
                # DatasetDictã®å ´åˆ
                for split in ['train', 'validation', 'test']:
                    if split in dataset:
                        all_data.extend(dataset[split])
            else:
                # å˜ä¸€ã®Datasetã®å ´åˆ
                all_data = list(dataset)
            
            output_file = f"{output_path}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {output_file}")
            print(f"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_data):,}")
        
        elif args.output_format == "jsonl":
            # JSONLå½¢å¼ã§ä¿å­˜
            output_file = f"{output_path}.jsonl"
            if args.compress:
                output_file += ".gz"
            
            # DatasetDictã®å ´åˆã¨Datasetã®å ´åˆã‚’å‡¦ç†
            from datasets import DatasetDict
            if isinstance(dataset, DatasetDict):
                # DatasetDictã®å ´åˆ
                data_dict = {'train': dataset['train'], 'validation': dataset['validation'], 'test': dataset['test']}
                num_samples = sum(len(dataset[split]) for split in ['train', 'validation', 'test'] if split in dataset)
            else:
                # å˜ä¸€ã®Datasetã®å ´åˆ
                data_dict = {'all': dataset}
                num_samples = len(dataset)
            
            DatasetSerializer.save_dataset(
                data=data_dict,
                metadata={'num_samples': num_samples},
                output_dir=output_path,
                format="jsonl",
                compress=args.compress
            )
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {output_file}")
        
        elif args.output_format == "parquet":
            # Parquetå½¢å¼ã§ä¿å­˜
            dataset.save_to_disk(output_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {output_path}")
        
        elif args.output_format == "csv":
            # CSVå½¢å¼ã§ä¿å­˜
            from datasets import DatasetDict
            if isinstance(dataset, DatasetDict):
                # DatasetDictã®å ´åˆ
                for split in ['train', 'validation', 'test']:
                    if split in dataset:
                        dataset[split].to_csv(f"{output_path}_{split}.csv")
            else:
                # å˜ä¸€ã®Datasetã®å ´åˆ
                dataset.to_csv(f"{output_path}.csv")
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {output_path}_*.csv")
    
    # ã‚¿ã‚¹ã‚¯åˆ†å¸ƒã®å‡¦ç†
    elif args.task_distribution:
        task_dist = {}
        for item in args.task_distribution.split(","):
            key, value = item.split(":")
            task_dist[key] = float(value)
    
        # é›£æ˜“åº¦åˆ†å¸ƒã®ãƒ‘ãƒ¼ã‚¹
        diff_dist = {}
        diff_mapping = {
            "beginner": TaskDifficulty.BEGINNER,
            "intermediate": TaskDifficulty.INTERMEDIATE,
            "advanced": TaskDifficulty.ADVANCED,
            "native": TaskDifficulty.NATIVE
        }
        for item in args.difficulty_distribution.split(","):
            key, value = item.split(":")
            diff_dist[diff_mapping[key]] = float(value)
        
        if args.preset == "all":
            # å…¨ãƒ—ãƒªã‚»ãƒƒãƒˆç”Ÿæˆ
            print("ğŸ¯ å…¨ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¾ã™")
            create_all_preset_datasets(args.output_dir)
        
        elif args.custom_size:
            # ã‚«ã‚¹ã‚¿ãƒ ã‚µã‚¤ã‚º
            print(f"ğŸ¯ ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ: {args.custom_size:,}ã‚¿ã‚¹ã‚¯")
            
            generator = LargeScaleDatasetGenerator(num_workers=args.num_workers)
            
            output_name = f"nihongo-dojo-custom-{args.custom_size}"
            output_path = os.path.join(args.output_dir, output_name)
            
            data, metadata = generator.generate_large_dataset(
                total_tasks=args.custom_size,
                task_type_distribution=task_dist,
                difficulty_distribution=diff_dist,
                seed=args.seed
            )
            
            DatasetSerializer.save_dataset(
                data=data,
                metadata=metadata,
                output_dir=output_path,
                format=args.output_format,
                compress=args.compress
            )
            
            print(f"âœ… ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {output_path}")
        
        else:
            # ãƒ—ãƒªã‚»ãƒƒãƒˆç”Ÿæˆ
            print(f"ğŸ¯ ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ: {args.preset}")
            
            output_path, metadata = generate_preset_dataset(
                preset=args.preset,
                output_dir=os.path.join(args.output_dir, f"nihongo-dojo-{args.preset}"),
                task_type_distribution=task_dist,
                difficulty_distribution=diff_dist,
                seed=args.seed
            )
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {output_path}")
            print(f"   ã‚¿ã‚¹ã‚¯æ•°: {metadata.num_tasks:,}")
            print(f"   ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {metadata.num_groups:,}")
    
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒªã‚»ãƒƒãƒˆå‡¦ç†
        preset = args.preset or "medium"
        print(f"ğŸ¯ ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ: {preset}")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯åˆ†å¸ƒ
        default_task_dist = {"basic": 0.6, "advanced": 0.3, "cultural": 0.1}
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é›£æ˜“åº¦åˆ†å¸ƒ
        default_diff_dist = {
            TaskDifficulty.BEGINNER: 0.25,
            TaskDifficulty.INTERMEDIATE: 0.35,
            TaskDifficulty.ADVANCED: 0.25,
            TaskDifficulty.NATIVE: 0.15
        }
        
        output_path, metadata = generate_preset_dataset(
            preset=preset,
            output_dir=os.path.join(args.output_dir, f"nihongo-dojo-{preset}"),
            task_type_distribution=default_task_dist,
            difficulty_distribution=default_diff_dist,
            seed=args.seed
        )
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†: {output_path}")
        print(f"   ã‚¿ã‚¹ã‚¯æ•°: {metadata.num_tasks:,}")
        print(f"   ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {metadata.num_groups:,}")


if __name__ == "__main__":
    main()